{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19da37b",
   "metadata": {},
   "source": [
    "# Data Cleaning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428476a2",
   "metadata": {},
   "source": [
    "With such a massive dataset, the cleaning process was both detailed and meticulous. Below, I provide a high-level overview of my general steps. More granular details, along with the specific justifications for my decisions, can be found within each corresponding section of this notebook.\n",
    "\n",
    "## Overview\n",
    "\n",
    "I cleaned each of the three datasets (crashes_df, people_df, and vehicles_df) with the primary goals of making the data more manageable and reducing noise by eliminating unnecessary features and handling null values.\n",
    "\n",
    "Throughout the cleaning process, I kept my target variable, most_severe_injury, in focus. Since most_severe_injury is a crash-level feature, the analysis in this project is centered around crash-level data. This distinction was especially important when working with people_df (person-level data) and vehicles_df (vehicle-level data), as merging these datasets with crashes_df required careful attention to avoid many-to-many relationships that could skew feature values. For instance, merging without proper aggregation could lead to inflated counts or inaccurate distributions of features such as the “number of injured persons per crash.” Without aggregation, a single crash with multiple people involved would be duplicated for each person in people_df, leading to an overrepresentation of crashes and skewed averages or totals. Proper aggregation ensures that each crash appears only once in the merged dataset\n",
    "\n",
    "To address this, I aggregated the cleaned people_df and vehicles_df into people_aggregated and vehicles_aggregated, respectively, before merging them with crashes_cleaned. This ensured a smooth merge process that maintained a one-to-one relationship with crash_record_id across the combined dataset.\n",
    "\n",
    "In order to simplify the analysis, I then adjusted the target variable (most_severe_injury) into a binary classification: serious injuries (fatalities and incapacitating injuries) versus non-serious injuries (minor or no injuries). This adjustment focused the modeling on predicting significant crash outcomes, providing a clearer and more manageable classification.\n",
    "\n",
    "The final merged dataset still contained over 600,000 records, which posed computational challenges. To address this, I employed stratified random sampling to create a representative subset of the data. This approach preserved the proportional distribution of classes in my target variable, most_severe_injury. To ensure reproducibility, I set a fixed random_state during sampling.\n",
    "\n",
    "The resulting stratified sample, ready for modeling, was uploaded to Kaggle to streamline reproducibility and accessibility for others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb3062",
   "metadata": {},
   "source": [
    "## 1.0 Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "210f18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for getting data\n",
    "import os\n",
    "import zipfile\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# for managing data\n",
    "import gc\n",
    "\n",
    "# for checking runtime\n",
    "import time\n",
    "\n",
    "# for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# for feature selection\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b01943",
   "metadata": {},
   "source": [
    "### 1.1 Environment Setup and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79cdc8",
   "metadata": {},
   "source": [
    "For reproducibility purposes, the next two code block allow the user to download the data for this project from kaggle.\n",
    "\n",
    "Please be aware that running this code will require you to enter your kaggle username and API key. The code will not proceed if you do not provide accurate information. \n",
    "\n",
    "To create a kaggle account, [click here](https://www.kaggle.com/account/login). \n",
    "\n",
    "For more details on obtaining your kaggle API key, [click here](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95c1719e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Kaggle username: ckucewicz\n",
      "Enter Kaggle key: 177a019058583e7df97a8ade860bbe3e\n",
      "Kaggle credentials are set up successfully.\n"
     ]
    }
   ],
   "source": [
    "# Getting Kaggle username and key from user input or environment variables\n",
    "os.environ[\"KAGGLE_USERNAME\"] = input(\"Enter Kaggle username: \")\n",
    "os.environ[\"KAGGLE_KEY\"] = input(\"Enter Kaggle key: \")\n",
    "\n",
    "# Detect the environment (Google Colab or local machine)\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # For Google Colab, use the /root/.kaggle directory\n",
    "    kaggle_path = Path('/root/.kaggle')\n",
    "else:\n",
    "    # For local machine, use the home directory\n",
    "    kaggle_path = Path.home() / '.kaggle'\n",
    "\n",
    "# Create the .kaggle directory if it doesn't exist\n",
    "os.makedirs(kaggle_path, exist_ok=True)\n",
    "\n",
    "# Create the kaggle.json file with the correct API credentials\n",
    "kaggle_json = {\n",
    "    \"username\": os.getenv(\"KAGGLE_USERNAME\"),\n",
    "    \"key\": os.getenv(\"KAGGLE_KEY\")\n",
    "}\n",
    "\n",
    "# Write the kaggle.json file in the correct location\n",
    "with open(kaggle_path / 'kaggle.json', 'w') as f:\n",
    "    json.dump(kaggle_json, f)\n",
    "\n",
    "# Set file permissions to secure the API key (optional but recommended)\n",
    "os.chmod(kaggle_path / 'kaggle.json', 0o600)\n",
    "\n",
    "# Check if the credentials are set correctly (for debugging purposes)\n",
    "print(\"Kaggle credentials are set up successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678c70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset in Jupyter Notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394M/394M [00:09<00:00, 44.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/ckucewicz/Chicago-Traffic-Data\n",
      "License(s): apache-2.0\n",
      "Downloading Chicago-Traffic-Data.zip to data\n",
      "\n",
      "Dataset extracted to: data/Chicago-Traffic-Data\n",
      "Loading people from data/Chicago-Traffic-Data/chicago_traffic_data/people.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriskucewicz/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508: DtypeWarning: Columns (19,23,24,25,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people DataFrame loaded successfully.\n",
      "Loading traffic_crashes from data/Chicago-Traffic-Data/chicago_traffic_data/traffic_crashes.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriskucewicz/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traffic_crashes DataFrame loaded successfully.\n",
      "Loading vehicles from data/Chicago-Traffic-Data/chicago_traffic_data/vehicles.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chriskucewicz/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3508: DtypeWarning: Columns (20,39,40,41,43,47,48,49,52,54,57,58,60,70) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicles DataFrame loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Download dataset using Kaggle API\n",
    "\n",
    "# Set dataset identifier and download path\n",
    "dataset_identifier = 'ckucewicz/Chicago-Traffic-Data'\n",
    "download_path = Path('./data')  # Local or Colab download folder\n",
    "\n",
    "# Ensure the download path exists\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Detect the environment and run the appropriate download command\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Downloading dataset in Google Colab...\")\n",
    "    !kaggle datasets download -d {dataset_identifier} --path {download_path}\n",
    "else:\n",
    "    print(\"Downloading dataset in Jupyter Notebook...\")\n",
    "    os.system(f\"kaggle datasets download -d {dataset_identifier} --path {download_path}\")\n",
    "\n",
    "# Step 3: Unzip the dataset\n",
    "zip_filename = download_path / 'Chicago-Traffic-Data.zip'  # Adjust the ZIP filename\n",
    "unzip_path = download_path / 'Chicago-Traffic-Data'\n",
    "\n",
    "# Ensure the extraction path exists\n",
    "os.makedirs(unzip_path, exist_ok=True)\n",
    "\n",
    "# Unzip the dataset\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(unzip_path)\n",
    "    print(f\"Dataset extracted to: {unzip_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {zip_filename} not found. Ensure the dataset was downloaded successfully.\")\n",
    "\n",
    "# Step 4: Load CSV files into pandas DataFrames\n",
    "csv_files = {\n",
    "    'people': 'chicago_traffic_data/people.csv',\n",
    "    'traffic_crashes': 'chicago_traffic_data/traffic_crashes.csv',\n",
    "    'vehicles': 'chicago_traffic_data/vehicles.csv',\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "for key, relative_path in csv_files.items():\n",
    "    csv_path = unzip_path / relative_path  # Create the full path\n",
    "    print(f\"Loading {key} from {csv_path}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load CSV into pandas DataFrame\n",
    "        dataframes[key] = pd.read_csv(csv_path, low_memory=True)\n",
    "        print(f\"{key} DataFrame loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {relative_path} not found in the extracted files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e708f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores each dataset in its own variable\n",
    "people_df = dataframes['people']\n",
    "traffic_crashes_df = dataframes['traffic_crashes']\n",
    "vehicles_df = dataframes['vehicles']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961925b",
   "metadata": {},
   "source": [
    "## 2.0 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bd0fb",
   "metadata": {},
   "source": [
    "### 2.1 Crashes\n",
    "\n",
    "The crashes dataset included my target variable, so I was careful with how I handled data cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce140e08",
   "metadata": {},
   "source": [
    "My steps included:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Dataset Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature Names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **Keep or drop features with remaining nulls**:\n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "    \n",
    "\n",
    "* **reduce feature cardinality with label reclassification**:\n",
    "\n",
    "    * `trafficway_type` and `lane_cnt`\n",
    "    * `crash_hour`, `crash_day_of_week`, `crash_month`\n",
    "    * `posted_speed_limit`\n",
    "    * `traffic_control_device`\n",
    "    * `prim_contributory_cause`\n",
    "    * `most_severe_injury`\n",
    " \n",
    " \n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (text variables as strings, numeric variables as int, categorical as category, ect.)\n",
    "    \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c9237f",
   "metadata": {},
   "source": [
    "#### 2.1.1 Preview Data: `.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72a9420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH_RECORD_ID</th>\n",
       "      <th>CRASH_DATE_EST_I</th>\n",
       "      <th>CRASH_DATE</th>\n",
       "      <th>POSTED_SPEED_LIMIT</th>\n",
       "      <th>TRAFFIC_CONTROL_DEVICE</th>\n",
       "      <th>DEVICE_CONDITION</th>\n",
       "      <th>WEATHER_CONDITION</th>\n",
       "      <th>LIGHTING_CONDITION</th>\n",
       "      <th>FIRST_CRASH_TYPE</th>\n",
       "      <th>TRAFFICWAY_TYPE</th>\n",
       "      <th>...</th>\n",
       "      <th>INJURIES_NON_INCAPACITATING</th>\n",
       "      <th>INJURIES_REPORTED_NOT_EVIDENT</th>\n",
       "      <th>INJURIES_NO_INDICATION</th>\n",
       "      <th>INJURIES_UNKNOWN</th>\n",
       "      <th>CRASH_HOUR</th>\n",
       "      <th>CRASH_DAY_OF_WEEK</th>\n",
       "      <th>CRASH_MONTH</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6c1659069e9c6285a650e70d6f9b574ed5f64c12888479...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08/18/2023 12:50:00 PM</td>\n",
       "      <td>15</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>FUNCTIONING PROPERLY</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>REAR END</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5f54a59fcb087b12ae5b1acff96a3caf4f2d37e79f8db4...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/29/2023 02:45:00 PM</td>\n",
       "      <td>30</td>\n",
       "      <td>TRAFFIC SIGNAL</td>\n",
       "      <td>FUNCTIONING PROPERLY</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PARKED MOTOR VEHICLE</td>\n",
       "      <td>DIVIDED - W/MEDIAN (NOT RAISED)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>41.854120</td>\n",
       "      <td>-87.665902</td>\n",
       "      <td>POINT (-87.665902342962 41.854120262952)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61fcb8c1eb522a6469b460e2134df3d15f82e81fd93e9c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08/18/2023 05:58:00 PM</td>\n",
       "      <td>30</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PEDALCYCLIST</td>\n",
       "      <td>NOT DIVIDED</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>41.942976</td>\n",
       "      <td>-87.761883</td>\n",
       "      <td>POINT (-87.761883496974 41.942975745006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004cd14d0303a9163aad69a2d7f341b7da2a8572b2ab33...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/26/2019 08:38:00 AM</td>\n",
       "      <td>25</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>PEDESTRIAN</td>\n",
       "      <td>ONE-WAY</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a1d5f0ea90897745365a4cbb06cc60329a120d89753fac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>08/18/2023 10:45:00 AM</td>\n",
       "      <td>20</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>NO CONTROLS</td>\n",
       "      <td>CLEAR</td>\n",
       "      <td>DAYLIGHT</td>\n",
       "      <td>FIXED OBJECT</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     CRASH_RECORD_ID CRASH_DATE_EST_I  \\\n",
       "0  6c1659069e9c6285a650e70d6f9b574ed5f64c12888479...              NaN   \n",
       "1  5f54a59fcb087b12ae5b1acff96a3caf4f2d37e79f8db4...              NaN   \n",
       "2  61fcb8c1eb522a6469b460e2134df3d15f82e81fd93e9c...              NaN   \n",
       "3  004cd14d0303a9163aad69a2d7f341b7da2a8572b2ab33...              NaN   \n",
       "4  a1d5f0ea90897745365a4cbb06cc60329a120d89753fac...              NaN   \n",
       "\n",
       "               CRASH_DATE  POSTED_SPEED_LIMIT TRAFFIC_CONTROL_DEVICE  \\\n",
       "0  08/18/2023 12:50:00 PM                  15                  OTHER   \n",
       "1  07/29/2023 02:45:00 PM                  30         TRAFFIC SIGNAL   \n",
       "2  08/18/2023 05:58:00 PM                  30            NO CONTROLS   \n",
       "3  11/26/2019 08:38:00 AM                  25            NO CONTROLS   \n",
       "4  08/18/2023 10:45:00 AM                  20            NO CONTROLS   \n",
       "\n",
       "       DEVICE_CONDITION WEATHER_CONDITION LIGHTING_CONDITION  \\\n",
       "0  FUNCTIONING PROPERLY             CLEAR           DAYLIGHT   \n",
       "1  FUNCTIONING PROPERLY             CLEAR           DAYLIGHT   \n",
       "2           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "3           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "4           NO CONTROLS             CLEAR           DAYLIGHT   \n",
       "\n",
       "       FIRST_CRASH_TYPE                  TRAFFICWAY_TYPE  ...  \\\n",
       "0              REAR END                            OTHER  ...   \n",
       "1  PARKED MOTOR VEHICLE  DIVIDED - W/MEDIAN (NOT RAISED)  ...   \n",
       "2          PEDALCYCLIST                      NOT DIVIDED  ...   \n",
       "3            PEDESTRIAN                          ONE-WAY  ...   \n",
       "4          FIXED OBJECT                            OTHER  ...   \n",
       "\n",
       "   INJURIES_NON_INCAPACITATING INJURIES_REPORTED_NOT_EVIDENT  \\\n",
       "0                          1.0                           0.0   \n",
       "1                          0.0                           0.0   \n",
       "2                          1.0                           0.0   \n",
       "3                          0.0                           0.0   \n",
       "4                          0.0                           0.0   \n",
       "\n",
       "  INJURIES_NO_INDICATION INJURIES_UNKNOWN CRASH_HOUR CRASH_DAY_OF_WEEK  \\\n",
       "0                    1.0              0.0         12                 6   \n",
       "1                    1.0              0.0         14                 7   \n",
       "2                    1.0              0.0         17                 6   \n",
       "3                    1.0              0.0          8                 3   \n",
       "4                    1.0              0.0         10                 6   \n",
       "\n",
       "  CRASH_MONTH   LATITUDE  LONGITUDE                                  LOCATION  \n",
       "0           8        NaN        NaN                                       NaN  \n",
       "1           7  41.854120 -87.665902  POINT (-87.665902342962 41.854120262952)  \n",
       "2           8  41.942976 -87.761883  POINT (-87.761883496974 41.942975745006)  \n",
       "3          11        NaN        NaN                                       NaN  \n",
       "4           8        NaN        NaN                                       NaN  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previews the first 5 rows of the dataframe\n",
    "traffic_crashes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45dc8ea",
   "metadata": {},
   "source": [
    "#### 2.1.2 Understand Structure: `.info()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce15ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 901446 entries, 0 to 901445\n",
      "Data columns (total 48 columns):\n",
      " #   Column                         Non-Null Count   Dtype  \n",
      "---  ------                         --------------   -----  \n",
      " 0   CRASH_RECORD_ID                901446 non-null  object \n",
      " 1   CRASH_DATE_EST_I               66531 non-null   object \n",
      " 2   CRASH_DATE                     901446 non-null  object \n",
      " 3   POSTED_SPEED_LIMIT             901446 non-null  int64  \n",
      " 4   TRAFFIC_CONTROL_DEVICE         901446 non-null  object \n",
      " 5   DEVICE_CONDITION               901446 non-null  object \n",
      " 6   WEATHER_CONDITION              901446 non-null  object \n",
      " 7   LIGHTING_CONDITION             901446 non-null  object \n",
      " 8   FIRST_CRASH_TYPE               901446 non-null  object \n",
      " 9   TRAFFICWAY_TYPE                901446 non-null  object \n",
      " 10  LANE_CNT                       199022 non-null  float64\n",
      " 11  ALIGNMENT                      901446 non-null  object \n",
      " 12  ROADWAY_SURFACE_COND           901446 non-null  object \n",
      " 13  ROAD_DEFECT                    901446 non-null  object \n",
      " 14  REPORT_TYPE                    873380 non-null  object \n",
      " 15  CRASH_TYPE                     901446 non-null  object \n",
      " 16  INTERSECTION_RELATED_I         207054 non-null  object \n",
      " 17  NOT_RIGHT_OF_WAY_I             41046 non-null   object \n",
      " 18  HIT_AND_RUN_I                  282692 non-null  object \n",
      " 19  DAMAGE                         901446 non-null  object \n",
      " 20  DATE_POLICE_NOTIFIED           901446 non-null  object \n",
      " 21  PRIM_CONTRIBUTORY_CAUSE        901446 non-null  object \n",
      " 22  SEC_CONTRIBUTORY_CAUSE         901446 non-null  object \n",
      " 23  STREET_NO                      901446 non-null  int64  \n",
      " 24  STREET_DIRECTION               901442 non-null  object \n",
      " 25  STREET_NAME                    901445 non-null  object \n",
      " 26  BEAT_OF_OCCURRENCE             901441 non-null  float64\n",
      " 27  PHOTOS_TAKEN_I                 12323 non-null   object \n",
      " 28  STATEMENTS_TAKEN_I             20743 non-null   object \n",
      " 29  DOORING_I                      2856 non-null    object \n",
      " 30  WORK_ZONE_I                    5035 non-null    object \n",
      " 31  WORK_ZONE_TYPE                 3886 non-null    object \n",
      " 32  WORKERS_PRESENT_I              1297 non-null    object \n",
      " 33  NUM_UNITS                      901446 non-null  int64  \n",
      " 34  MOST_SEVERE_INJURY             899453 non-null  object \n",
      " 35  INJURIES_TOTAL                 899467 non-null  float64\n",
      " 36  INJURIES_FATAL                 899467 non-null  float64\n",
      " 37  INJURIES_INCAPACITATING        899467 non-null  float64\n",
      " 38  INJURIES_NON_INCAPACITATING    899467 non-null  float64\n",
      " 39  INJURIES_REPORTED_NOT_EVIDENT  899467 non-null  float64\n",
      " 40  INJURIES_NO_INDICATION         899467 non-null  float64\n",
      " 41  INJURIES_UNKNOWN               899467 non-null  float64\n",
      " 42  CRASH_HOUR                     901446 non-null  int64  \n",
      " 43  CRASH_DAY_OF_WEEK              901446 non-null  int64  \n",
      " 44  CRASH_MONTH                    901446 non-null  int64  \n",
      " 45  LATITUDE                       894920 non-null  float64\n",
      " 46  LONGITUDE                      894920 non-null  float64\n",
      " 47  LOCATION                       894920 non-null  object \n",
      "dtypes: float64(11), int64(6), object(31)\n",
      "memory usage: 330.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# provides info about the dataframe features, non-null values, and datatypes\n",
    "traffic_crashes_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b43520",
   "metadata": {},
   "source": [
    "#### 2.1.3 Format Feature names and Row Values: `.lower()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebd316",
   "metadata": {},
   "source": [
    "From the first two steps of previewing the data and understanding its structure, two things stand out: There is A LOT of data, and it is messy with varying amounts of null values in each feature. I decided to make the feature names lower case simply for readability, and as a precuror cleaning step I made all string values lower case in the hopes that this may deal with some label misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fe2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts all feature names to lower case \n",
    "traffic_crashes_df.columns = traffic_crashes_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4492401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in traffic_crashes_df.select_dtypes(include='object').columns:\n",
    "    traffic_crashes_df[col] = traffic_crashes_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3a0a1",
   "metadata": {},
   "source": [
    "#### 2.1.4 Drop features with overly high null values: \n",
    "\n",
    "While there are a lot of records in this dataframe, some features also have a lot of null values.  I know that features with a significant majority of null values will not be helpful for analysis so I start by trying to identify these features and remove them right off the bat. Rather than look at the count of nulls, I used `(.isna().sum()/ len(df)) *100` to get the percentage of nulls for each feature. This is easier to grasp than null counts in the 10 and hundred thousands. \n",
    "\n",
    "For quick cleaning, I chose a 90% null threshold to automatically remove features. For the features with between 60-90% nulls, I decided to inspect them more closely before blinding removing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d82960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id                   0.00\n",
       "crash_date_est_i                 92.62\n",
       "crash_date                        0.00\n",
       "posted_speed_limit                0.00\n",
       "traffic_control_device            0.00\n",
       "device_condition                  0.00\n",
       "weather_condition                 0.00\n",
       "lighting_condition                0.00\n",
       "first_crash_type                  0.00\n",
       "trafficway_type                   0.00\n",
       "lane_cnt                         77.92\n",
       "alignment                         0.00\n",
       "roadway_surface_cond              0.00\n",
       "road_defect                       0.00\n",
       "report_type                       3.11\n",
       "crash_type                        0.00\n",
       "intersection_related_i           77.03\n",
       "not_right_of_way_i               95.45\n",
       "hit_and_run_i                    68.64\n",
       "damage                            0.00\n",
       "date_police_notified              0.00\n",
       "prim_contributory_cause           0.00\n",
       "sec_contributory_cause            0.00\n",
       "street_no                         0.00\n",
       "street_direction                  0.00\n",
       "street_name                       0.00\n",
       "beat_of_occurrence                0.00\n",
       "photos_taken_i                   98.63\n",
       "statements_taken_i               97.70\n",
       "dooring_i                        99.68\n",
       "work_zone_i                      99.44\n",
       "work_zone_type                   99.57\n",
       "workers_present_i                99.86\n",
       "num_units                         0.00\n",
       "most_severe_injury                0.22\n",
       "injuries_total                    0.22\n",
       "injuries_fatal                    0.22\n",
       "injuries_incapacitating           0.22\n",
       "injuries_non_incapacitating       0.22\n",
       "injuries_reported_not_evident     0.22\n",
       "injuries_no_indication            0.22\n",
       "injuries_unknown                  0.22\n",
       "crash_hour                        0.00\n",
       "crash_day_of_week                 0.00\n",
       "crash_month                       0.00\n",
       "latitude                          0.72\n",
       "longitude                         0.72\n",
       "location                          0.72\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stores the percentages of null values for each feature, rounded to 2 places, in a variable\n",
    "missing_percentage = round((traffic_crashes_df.isna().sum()/len(traffic_crashes_df)*100), 2)\n",
    "missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d470c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "high_null_features = traffic_crashes_df.columns[(traffic_crashes_df.isna().sum() / len(traffic_crashes_df) * 100) >= 90]\n",
    "high_null_features\n",
    "\n",
    "# creating a list of features with 90% or more null values\n",
    "high_null_features_list = list(high_null_features)\n",
    "high_null_features_list\n",
    "\n",
    "crashes_cleaned = traffic_crashes_df.drop(columns=high_null_features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3f1a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['crash_date_est_i', 'not_right_of_way_i', 'photos_taken_i',\n",
       "       'statements_taken_i', 'dooring_i', 'work_zone_i', 'work_zone_type',\n",
       "       'workers_present_i'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all features with 90% or more of its values are null\n",
    "high_null_features = traffic_crashes_df.columns[(traffic_crashes_df.isna().sum() / len(traffic_crashes_df) * 100) >= 90]\n",
    "high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb444f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crash_date_est_i',\n",
       " 'not_right_of_way_i',\n",
       " 'photos_taken_i',\n",
       " 'statements_taken_i',\n",
       " 'dooring_i',\n",
       " 'work_zone_i',\n",
       " 'work_zone_type',\n",
       " 'workers_present_i']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a list of features with 90% or more null values\n",
    "high_null_features_list = list(high_null_features)\n",
    "high_null_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "423cdbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all features with 90% or more null values, saves the new datafram as crashes_cleaned\n",
    "crashes_cleaned = traffic_crashes_df.drop(columns=high_null_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d243e7c",
   "metadata": {},
   "source": [
    "#### 2.1.5 Check for duplicates: `.duplicated().sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ca6bb",
   "metadata": {},
   "source": [
    "Checking for duplicate rows is another common cleaning step. In this case, there were no duplicate rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6a755f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate crash_record_id values in crashes_cleaned: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate crash_record_id values in crashes_cleaned\n",
    "print(f\"Number of duplicate crash_record_id values in crashes_cleaned: {crashes_cleaned['crash_record_id'].duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096086a",
   "metadata": {},
   "source": [
    "#### 2.1.6 Keep or drop features with remaining nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32b51a",
   "metadata": {},
   "source": [
    "As mentioned earlier, I wanted at least look at features with between 60-90% null values before removing them so I inspected the value_counts() for these 3 features:\n",
    "* `hit_and_run_i` describes the aftermath of the crash, not a contributing factor so I drop this\n",
    "* `intersection_related_i` initially seems like it could be a good feature to keep, but upon further inspection, it appears to be vague. CDOT's definition: \"A field observation by the police officer whether an intersection played a role in the crash. Does not represent whether or not the crash occurred within the intersection\", makes me feel confident removing this as it seems like it is ultimally up to the officer's disgretion, which may not lead to useful analysis, so I dropped it. \n",
    "* `lane_cnt`: Despite the high percentage of null values I decided to keep this. My domain knowledge makes me think the number of lanes could affect how cautiously or uncautiously drivers driver. Drivers generally will drive more cautiously down a narrow two or one lane, than on an open 6 lane freeway. \n",
    "\n",
    "Once I created a new dataframe, leaving out features I removed, no longer needed the original dataset so I deleted it from memory due to its large size and ability to take up a large amount of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b27cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves all features with between 60%-90% null values as a variable\n",
    "\n",
    "medium_null_features = traffic_crashes_df.columns[\n",
    "    ((traffic_crashes_df.isna().sum() / len(traffic_crashes_df) * 100) >= 60) &\n",
    "    ((traffic_crashes_df.isna().sum() / len(traffic_crashes_df) * 100) < 90)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5411b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts the above variable to list type\n",
    "medium_null_features_list = list(medium_null_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30be9b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for column 'lane_cnt':\n",
      "2.0          91162\n",
      "4.0          49589\n",
      "1.0          32550\n",
      "3.0           8678\n",
      "0.0           8032\n",
      "6.0           4502\n",
      "5.0           1940\n",
      "8.0           1908\n",
      "7.0            184\n",
      "10.0           162\n",
      "99.0           108\n",
      "9.0             66\n",
      "11.0            30\n",
      "12.0            29\n",
      "20.0            15\n",
      "22.0            13\n",
      "15.0             7\n",
      "16.0             7\n",
      "14.0             5\n",
      "30.0             5\n",
      "40.0             4\n",
      "60.0             3\n",
      "21.0             3\n",
      "25.0             2\n",
      "100.0            2\n",
      "902.0            1\n",
      "24.0             1\n",
      "80.0             1\n",
      "218474.0         1\n",
      "45.0             1\n",
      "17.0             1\n",
      "299679.0         1\n",
      "19.0             1\n",
      "400.0            1\n",
      "13.0             1\n",
      "1191625.0        1\n",
      "35.0             1\n",
      "433634.0         1\n",
      "41.0             1\n",
      "28.0             1\n",
      "44.0             1\n",
      "Name: lane_cnt, dtype: int64\n",
      "--------------------------------\n",
      "Value counts for column 'intersection_related_i':\n",
      "y    197181\n",
      "n      9873\n",
      "Name: intersection_related_i, dtype: int64\n",
      "--------------------------------\n",
      "Value counts for column 'hit_and_run_i':\n",
      "y    270587\n",
      "n     12105\n",
      "Name: hit_and_run_i, dtype: int64\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# iterates through list of medium null features and prints the value_counts() for each\n",
    "for feature in medium_null_features_list:\n",
    "    print(f\"Value counts for column '{feature}':\")\n",
    "    print(crashes_cleaned[feature].value_counts())\n",
    "    print(\"-\"* 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4fbfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes the traffic_crashes_df dataframe to clear up memory\n",
    "\n",
    "del traffic_crashes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9909e46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3e660",
   "metadata": {},
   "source": [
    "#### 2.1.7 Inspect remaining features: `.value_counts()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ea1a3",
   "metadata": {},
   "source": [
    "Once columns with null values were taken care of, I used `.value_counts()` to inspect many of the remaining columns to look for things like cardianlity, misspelling, class labels, stored data types, etc. I used some domain knowledge to avoid looking through every single feature. \n",
    "\n",
    "Some features that I decided to keep because they might be helpful in predicting my target had unclear labels or a high number of labels (aka high cardinality). In order to make sure these features were not only useful for analysis, but also ready for the modeling phase, I used some domain knowledge to help reclassify some of the feature labels into less, more easily understandable labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91b90416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.976201    1438\n",
       "41.900959     817\n",
       "41.791420     628\n",
       "41.751461     615\n",
       "41.722257     489\n",
       "             ... \n",
       "41.812116       1\n",
       "41.742553       1\n",
       "41.757199       1\n",
       "41.936622       1\n",
       "41.951640       1\n",
       "Name: latitude, Length: 319343, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within latitude feature\n",
    "crashes_cleaned['latitude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "416ddbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-87.905309    1438\n",
       "-87.619928     817\n",
       "-87.580148     628\n",
       "-87.585972     615\n",
       "-87.585276     489\n",
       "              ... \n",
       "-87.610788       1\n",
       "-87.555370       1\n",
       "-87.647808       1\n",
       "-87.616396       1\n",
       "-87.732269       1\n",
       "Name: longitude, Length: 319308, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within longitude feature\n",
    "crashes_cleaned['longitude'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52286192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "point (-87.905309125103 41.976201139024)    1438\n",
       "point (-87.619928173678 41.900958919109)     817\n",
       "point (-87.580147768689 41.791420282098)     628\n",
       "point (-87.585971992965 41.751460603167)     615\n",
       "point (-87.585275565077 41.722257273006)     489\n",
       "                                            ... \n",
       "point (-87.639121523275 41.869503004763)       1\n",
       "point (-87.602411667064 41.804009965883)       1\n",
       "point (-87.674723215169 41.96342865936)        1\n",
       "point (-87.755347401896 41.896861551221)       1\n",
       "point (-87.732268503259 41.951640180538)       1\n",
       "Name: location, Length: 319546, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within location feature\n",
    "crashes_cleaned['location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f749178",
   "metadata": {},
   "source": [
    "While location data seems like it be very insightful, the cardinality is simply too high to help with predictive capacity, and will ultimately restrict my limited computing power. With more time, in future iterations of this project I hope to be able to process the location data to be insightful and able to be kept, but for the purposes of a minimum viable product, I'll remove lat, long, and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c07f3b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30    664045\n",
       "35     59626\n",
       "25     57789\n",
       "20     37717\n",
       "15     32112\n",
       "10     21096\n",
       "40      8612\n",
       "0       7584\n",
       "45      5951\n",
       "5       4957\n",
       "55       883\n",
       "50       276\n",
       "3        221\n",
       "9         96\n",
       "39        95\n",
       "99        66\n",
       "60        53\n",
       "1         41\n",
       "24        38\n",
       "2         31\n",
       "65        20\n",
       "32        20\n",
       "34        16\n",
       "33        14\n",
       "11        11\n",
       "26        11\n",
       "36         8\n",
       "6          7\n",
       "70         7\n",
       "7          6\n",
       "18         4\n",
       "12         4\n",
       "22         4\n",
       "14         4\n",
       "23         3\n",
       "29         3\n",
       "31         2\n",
       "8          2\n",
       "38         2\n",
       "16         2\n",
       "4          2\n",
       "62         1\n",
       "63         1\n",
       "44         1\n",
       "49         1\n",
       "46         1\n",
       "Name: posted_speed_limit, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within posted_speed_limit feature\n",
    "crashes_cleaned['posted_speed_limit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213eadb3",
   "metadata": {},
   "source": [
    "This feature could be important, but will require cardinality reduction. To help make this feature better able to be handled for modeling I decided to reclassify this feature into 3 speed groups: low (**0-25** mph), medium (**25-40** mph), and high (**40+ mph**). \n",
    "\n",
    "These decisions were intentional as both reports from Philadelphia's and Chicago's transportation department site speed as a key factor with speeds greater than 40 resulting in high probability of death for pedestrians if hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92f8386b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Medium    732454\n",
       "Low       161731\n",
       "High        7261\n",
       "Name: speed_limit_category, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorize speed limits directly without using a function\n",
    "crashes_cleaned['speed_limit_category'] = pd.cut(\n",
    "    crashes_cleaned['posted_speed_limit'],\n",
    "    bins=[-float('inf'), 25, 40, float('inf')],\n",
    "    labels=['Low', 'Medium', 'High'],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "crashes_cleaned['speed_limit_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fde7379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no controls                 510287\n",
       "traffic signal              249882\n",
       "stop sign/flasher            89361\n",
       "unknown                      38328\n",
       "other                         6096\n",
       "yield                         1365\n",
       "lane use marking              1226\n",
       "other reg. sign               1103\n",
       "other warning sign             715\n",
       "pedestrian crossing sign       636\n",
       "railroad crossing gate         581\n",
       "flashing control signal        373\n",
       "school zone                    353\n",
       "delineators                    352\n",
       "police/flagman                 309\n",
       "rr crossing sign               195\n",
       "other railroad crossing        192\n",
       "no passing                      58\n",
       "bicycle crossing sign           34\n",
       "Name: traffic_control_device, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within traffic_control_device feature\n",
    "crashes_cleaned['traffic_control_device'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e033658",
   "metadata": {},
   "source": [
    "During my time working with Philadelphia's Vision Zero department, I learned that traffic light signals have the potential to increase chances of speeding as opposed to stop signs, mainly in the context of drivers speeding up at yellow lights to not get stuck at the light. With this knowledge, I feel like this feature could be an insightful predictor, but will need to reduce its cardinality. I reclassified this feature into 4 categories: Signal, sign, markings & lanes, and other. Reduced cardinality will reduce the dimensionality during OneHotEncoding, leading to improved computational efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1e7867f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other               555212\n",
       "Signal              251472\n",
       "Sign                 91886\n",
       "Markings & Lanes      1578\n",
       "Name: traffic_control_category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map the original 'traffic_control_device' values to more specific categories\n",
    "traffic_control_mapping = {\n",
    "    'traffic signal': 'Signal',\n",
    "    'flashing control signal': 'Signal',\n",
    "    'pedestrian crossing sign': 'Signal',  # If it's a signal\n",
    "    'railroad crossing gate': 'Signal',    # If it uses lights\n",
    "    \n",
    "    'stop sign/flasher': 'Sign',\n",
    "    'yield': 'Sign',\n",
    "    'school zone': 'Sign',\n",
    "    'railroad crossing sign': 'Sign',      # If static sign\n",
    "    'other warning sign': 'Sign',\n",
    "    'bicycle crossing sign': 'Sign',\n",
    "    'no passing': 'Sign',\n",
    "    \n",
    "    'lane use marking': 'Markings & Lanes',\n",
    "    'delineators': 'Markings & Lanes',\n",
    "    \n",
    "    'no controls': 'Other',\n",
    "    'unknown': 'Other',\n",
    "    'other': 'Other',\n",
    "    'police/flagman': 'Other',\n",
    "    'other railroad crossing': 'Other'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'traffic_control_device' column\n",
    "crashes_cleaned['traffic_control_category'] = crashes_cleaned['traffic_control_device'].map(traffic_control_mapping)\n",
    "\n",
    "# Check the value counts for the new grouped categories\n",
    "crashes_cleaned['traffic_control_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fbb26238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no controls                 516329\n",
       "functioning properly        307784\n",
       "unknown                      63428\n",
       "other                         6836\n",
       "functioning improperly        4113\n",
       "not functioning               2562\n",
       "worn reflective material       295\n",
       "missing                         99\n",
       "Name: device_condition, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within device_condition feature\n",
    "crashes_cleaned['device_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e6d4a",
   "metadata": {},
   "source": [
    "The top two categories that make up the majority of this feature are 'no controls' and 'functioning properly'. Then the next two frequent classes are 'unknown' and 'other'. Due to this, this feature does not feel like it will be particularly insightful, so we can drop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40f2d792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "daylight                  578548\n",
       "darkness, lighted road    197098\n",
       "unknown                    42569\n",
       "darkness                   42455\n",
       "dusk                       25737\n",
       "dawn                       15039\n",
       "Name: lighting_condition, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within lighting_condition feature\n",
    "crashes_cleaned['lighting_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efbfb0",
   "metadata": {},
   "source": [
    "This seems like it could offer insightful predictive capacity, and the cardinality is not high. I will keep this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49de98a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parked motor vehicle            208646\n",
       "rear end                        199321\n",
       "sideswipe same direction        138501\n",
       "turning                         129668\n",
       "angle                            97996\n",
       "fixed object                     41874\n",
       "pedestrian                       21320\n",
       "pedalcyclist                     14331\n",
       "sideswipe opposite direction     12509\n",
       "rear to front                     9252\n",
       "other object                      8981\n",
       "head on                           7639\n",
       "rear to side                      5512\n",
       "other noncollision                2745\n",
       "rear to rear                      1907\n",
       "animal                             655\n",
       "overturned                         543\n",
       "train                               46\n",
       "Name: first_crash_type, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within first_crash_type feature\n",
    "crashes_cleaned['first_crash_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf2141",
   "metadata": {},
   "source": [
    "This information seems like it could be somewhat potentially insightful, but I feel other features will offer better insights. So in an effort to reduce the dataset to only the most useful features, I will remove this feature. This could be a feature to include in future iterations of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b3d7be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not divided                        388246\n",
       "divided - w/median (not raised)    142466\n",
       "one-way                            114072\n",
       "four way                            62561\n",
       "parking lot                         61011\n",
       "divided - w/median barrier          50946\n",
       "other                               24388\n",
       "alley                               14802\n",
       "t-intersection                      12409\n",
       "unknown                             10603\n",
       "center turn lane                     6374\n",
       "driveway                             2890\n",
       "ramp                                 2834\n",
       "unknown intersection type            2762\n",
       "five point, or more                  1385\n",
       "y-intersection                       1350\n",
       "traffic route                        1166\n",
       "not reported                          687\n",
       "roundabout                            308\n",
       "l-intersection                        186\n",
       "Name: trafficway_type, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within trafficway_type feature\n",
    "crashes_cleaned['trafficway_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59928afc",
   "metadata": {},
   "source": [
    "This information feels like it could be important but I will keep this feature. With some label reclassification I reduce this feature down to 7 categories, making for a more insightful and computationally efficient feature for prediction. The new reclassified feature's name is 'road_category' so I will keep it and drop the original trafficway_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2fc4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define intersection types\n",
    "intersection_types = ['roundabout', 'l-intersection', 'y-intersection', \n",
    "                      'five point, or more', 'center turn lane', \n",
    "                      't-intersection', 'unknown intersection type']\n",
    "\n",
    "# Define conditions for both blocks (with block 2 modification)\n",
    "conditions = [\n",
    "    (crashes_cleaned['trafficway_type'] == 'one-way') & (crashes_cleaned['lane_cnt'] == 1),\n",
    "    (crashes_cleaned['trafficway_type'] == 'one-way') & (crashes_cleaned['lane_cnt'] > 1),\n",
    "    (crashes_cleaned['trafficway_type'].isin(intersection_types)),\n",
    "    (crashes_cleaned['trafficway_type'].isin(['unknown', 'not reported'])) | \n",
    "    (pd.isnull(crashes_cleaned['trafficway_type'])) | \n",
    "    (pd.isnull(crashes_cleaned['lane_cnt'])),\n",
    "    (crashes_cleaned['trafficway_type'].isin(['parking lot', 'driveway', 'ramp', 'alley', 'other'])),\n",
    "    # Modified condition for 'multi-lane bidirectional' from Block 2\n",
    "    (crashes_cleaned['lane_cnt'] > 1) & \n",
    "    (~crashes_cleaned['trafficway_type'].isin([\n",
    "        'one-way', 'four way', 'unknown', 'not reported', \n",
    "        'other', 'parking lot', 'driveway', 'ramp', 'alley'\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# Define corresponding categories\n",
    "choices = [\n",
    "    'single-lane one way',\n",
    "    'multi-lane one way',\n",
    "    'intersection',\n",
    "    'unknown',  # Combined \"unknown\" and \"not reported\"\n",
    "    'other',\n",
    "    'multi-lane bidirectional'\n",
    "]\n",
    "\n",
    "# Apply classification\n",
    "crashes_cleaned['road_category'] = np.select(conditions, choices, default='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74dc675a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unknown                     689310\n",
       "multi-lane bidirectional    138918\n",
       "intersection                 24774\n",
       "other                        19589\n",
       "single-lane one way          17992\n",
       "multi-lane one way           10863\n",
       "Name: road_category, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of categories in the new column\n",
    "crashes_cleaned['road_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44e2e5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "straight and level       880103\n",
       "straight on grade         11022\n",
       "curve, level               6352\n",
       "straight on hillcrest      2267\n",
       "curve on grade             1313\n",
       "curve on hillcrest          389\n",
       "Name: alignment, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within alignment feature\n",
    "crashes_cleaned['alignment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4681580",
   "metadata": {},
   "source": [
    "While this feature would ideally be helpful as a predictor, the data here is not conducive for analysis. Most of the entries are 'straight and level', so I will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5c5d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clear                       709235\n",
       "rain                         77962\n",
       "unknown                      51500\n",
       "snow                         28844\n",
       "cloudy/overcast              26333\n",
       "other                         2789\n",
       "freezing rain/drizzle         1787\n",
       "fog/smoke/haze                1353\n",
       "sleet/hail                    1026\n",
       "blowing snow                   453\n",
       "severe cross wind gate         156\n",
       "blowing sand, soil, dirt         8\n",
       "Name: weather_condition, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within weather_condition feature\n",
    "crashes_cleaned['weather_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff59ff9",
   "metadata": {},
   "source": [
    "This feature seems important, but will have to compare it to roadway_surface_cond feature as they both contain similar information. I will make a decision about which will be most insightful, and drop the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a56ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dry                667224\n",
       "wet                117323\n",
       "unknown             80085\n",
       "snow or slush       28524\n",
       "ice                  5678\n",
       "other                2290\n",
       "sand, mud, dirt       322\n",
       "Name: roadway_surface_cond, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within roadway_surface_cond feature\n",
    "crashes_cleaned['roadway_surface_cond'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3befae",
   "metadata": {},
   "source": [
    "This is somewhat redundant with weather condition. I chose to remove weather_condition  and keep roadway_surface_cond due to its lower cardinality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19538aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w    322771\n",
       "s    301079\n",
       "n    216752\n",
       "e     60840\n",
       "Name: street_direction, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within street_direction feature\n",
    "crashes_cleaned['street_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d4e88",
   "metadata": {},
   "source": [
    "Even though it feel like there could be some predictive power with this feature, I chose to drop it as I think it will not be as insightful as other features, and with the vast amount of features across 3 datasets, I am only trying to keep the ones I feel are most important. With more time I could potentially run a simple decision tree and obtain the feature_importances to help with this decision, but with an approaching deadline I opt to simply remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fd33d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "western ave        24619\n",
       "pulaski rd         21778\n",
       "cicero ave         20285\n",
       "ashland ave        19606\n",
       "halsted st         17440\n",
       "                   ...  \n",
       "franklin sd            1\n",
       "lacey ave              1\n",
       "stetson sub ave        1\n",
       "11th pl                1\n",
       "29th pl                1\n",
       "Name: street_name, Length: 1648, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within street_name feature\n",
    "crashes_cleaned['street_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194626d",
   "metadata": {},
   "source": [
    "Again I feel like this could offer some predictive insights, but due to its high cardinality I will remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c10f89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no defects           718022\n",
       "unknown              166233\n",
       "rut, holes             6350\n",
       "other                  4893\n",
       "worn surface           3741\n",
       "shoulder defect        1547\n",
       "debris on roadway       660\n",
       "Name: road_defect, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within road_defect feature\n",
    "crashes_cleaned['road_defect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb64d23e",
   "metadata": {},
   "source": [
    "The main two labels here are \"no defects\" and unknown. This will not be helpful for analysis, so I will remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6858cb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no injury / drive away              658842\n",
       "injury and / or tow due to crash    242604\n",
       "Name: crash_type, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "crashes_cleaned['crash_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72031c1f",
   "metadata": {},
   "source": [
    "This feature describes the aftermath of the crash which is not helpful for this model. I will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9066bb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1834.0    10913\n",
       "114.0      9281\n",
       "813.0      9093\n",
       "815.0      8590\n",
       "1831.0     8244\n",
       "          ...  \n",
       "1653.0      502\n",
       "1655.0      313\n",
       "1652.0      241\n",
       "1650.0       69\n",
       "6100.0        7\n",
       "Name: beat_of_occurrence, Length: 276, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within beat_of_occurrence feature\n",
    "crashes_cleaned['beat_of_occurrence'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef0601",
   "metadata": {},
   "source": [
    "Another potentially insightful feature but with high cardinality, and limited computing power, I choose to remove this. In future iterations, it would be interesting to inspect this feature further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e597a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no indication of injury     772801\n",
       "nonincapacitating injury     71130\n",
       "reported, not evident        39463\n",
       "incapacitating injury        15074\n",
       "fatal                          985\n",
       "Name: most_severe_injury, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within most_severe_injury feature\n",
    "crashes_cleaned['most_severe_injury'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93567f46",
   "metadata": {},
   "source": [
    "This is my target variable, so I will keep it, but to improve its interpretability and ensure it is aligned with the problem context of predicting fatal or serious injuries, I reclassify it to have two classes: non-serious injury and serious injury. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70104eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the string 'nan' with actual NaN values\n",
    "crashes_cleaned['most_severe_injury'] = crashes_cleaned['most_severe_injury'].replace('nan', np.nan)\n",
    "\n",
    "# Now categorize the injuries into 'Serious' and 'Non-serious'\n",
    "crashes_cleaned['severity_category'] = crashes_cleaned['most_severe_injury'].replace({\n",
    "    'no indication of injury': 'Non-serious',\n",
    "    'nonincapacitating injury': 'Non-serious',\n",
    "    'reported, not evident': 'Non-serious',\n",
    "    'incapacitating injury': 'Serious',\n",
    "    'fatal': 'Serious'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0227a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     772815\n",
       "1.0      95189\n",
       "2.0      21269\n",
       "3.0       6479\n",
       "4.0       2302\n",
       "5.0        825\n",
       "6.0        325\n",
       "7.0        133\n",
       "8.0         53\n",
       "9.0         27\n",
       "10.0        16\n",
       "11.0         9\n",
       "15.0         8\n",
       "12.0         6\n",
       "21.0         4\n",
       "13.0         3\n",
       "17.0         1\n",
       "14.0         1\n",
       "19.0         1\n",
       "16.0         1\n",
       "Name: injuries_total, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within injuries_total feature\n",
    "crashes_cleaned['injuries_total'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed22158",
   "metadata": {},
   "source": [
    "This information is redundant with my target so I choose to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d371c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    898482\n",
       "1.0       912\n",
       "2.0        64\n",
       "3.0         8\n",
       "4.0         1\n",
       "Name: injuries_fatal, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within injuries_fatal feature\n",
    "crashes_cleaned['injuries_fatal'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e87cd03",
   "metadata": {},
   "source": [
    "This information is redundant with my target so I choose to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "395c1f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     884243\n",
       "1.0      13370\n",
       "2.0       1395\n",
       "3.0        312\n",
       "4.0        107\n",
       "5.0         29\n",
       "6.0          7\n",
       "7.0          2\n",
       "10.0         1\n",
       "8.0          1\n",
       "Name: injuries_incapacitating, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within injuries_incapacitating feature\n",
    "crashes_cleaned['injuries_incapacitating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee88fc7",
   "metadata": {},
   "source": [
    "This information is redundant with my target so I choose to drop it. This information is captured in the 'most_severe_injury' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27290585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unable to determine                                                                 352689\n",
       "failing to yield right-of-way                                                        99589\n",
       "following too closely                                                                86950\n",
       "not applicable                                                                       47632\n",
       "improper overtaking/passing                                                          44963\n",
       "failing to reduce speed to avoid crash                                               37868\n",
       "improper backing                                                                     34796\n",
       "improper lane usage                                                                  32108\n",
       "driving skills/knowledge/experience                                                  30632\n",
       "improper turning/no signal                                                           30203\n",
       "disregarding traffic signals                                                         17608\n",
       "weather                                                                              12961\n",
       "operating vehicle in erratic, reckless, careless, negligent or aggressive manner     11347\n",
       "disregarding stop sign                                                                9609\n",
       "distraction - from inside vehicle                                                     6092\n",
       "equipment - vehicle condition                                                         5576\n",
       "physical condition of driver                                                          5305\n",
       "vision obscured (signs, tree limbs, buildings, etc.)                                  5116\n",
       "driving on wrong side/wrong way                                                       4885\n",
       "under the influence of alcohol/drugs (use when arrest is effected)                    4183\n",
       "distraction - from outside vehicle                                                    3613\n",
       "road engineering/surface/marking defects                                              2129\n",
       "exceeding authorized speed limit                                                      1982\n",
       "disregarding other traffic signs                                                      1914\n",
       "road construction/maintenance                                                         1884\n",
       "exceeding safe speed for conditions                                                   1684\n",
       "evasive action due to animal, object, nonmotorist                                     1627\n",
       "cell phone use other than texting                                                     1188\n",
       "disregarding road markings                                                            1106\n",
       "had been drinking (use when arrest is not made)                                        905\n",
       "animal                                                                                 766\n",
       "turning right on red                                                                   687\n",
       "related to bus stop                                                                    481\n",
       "distraction - other electronic device (navigation device, dvd player, etc.)            425\n",
       "texting                                                                                343\n",
       "disregarding yield sign                                                                279\n",
       "passing stopped school bus                                                             110\n",
       "obstructed crosswalks                                                                  100\n",
       "bicycle advancing legally on red light                                                  88\n",
       "motorcycle advancing legally on red light                                               23\n",
       "Name: prim_contributory_cause, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within prim_contributory_cause feature\n",
    "crashes_cleaned['prim_contributory_cause'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa819250",
   "metadata": {},
   "source": [
    "This feature feels like it is particularly insightful so I will keep it, but will perform label reclassification to better prepare it for modeling by reducing cardinality, and improve interpretability using more understandable grouping labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20cc3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping for the primary contributory causes\n",
    "cause_mapping = {\n",
    "    'distraction - from inside vehicle': 'Distraction',\n",
    "    'distraction - from outside vehicle': 'Distraction',\n",
    "    'cell phone use other than texting': 'Distraction',\n",
    "    'distraction - other electronic device (navigation device, dvd player, etc.)': 'Distraction',\n",
    "    'texting': 'Distraction',\n",
    "    'bicycle advancing legally on red light': 'Distraction',\n",
    "    'motorcycle advancing legally on red light': 'Distraction',\n",
    "    \n",
    "    'operating vehicle in erratic, reckless, careless, negligent or aggressive manner': 'Aggressive/Reckless Driving',\n",
    "    'failing to reduce speed to avoid crash': 'Aggressive/Reckless Driving',\n",
    "    'exceeding authorized speed limit': 'Aggressive/Reckless Driving',\n",
    "    'exceeding safe speed for conditions': 'Aggressive/Reckless Driving',\n",
    "    'driving on wrong side/wrong way': 'Aggressive/Reckless Driving',\n",
    "    'disregarding stop sign': 'Aggressive/Reckless Driving',\n",
    "    'disregarding traffic signals': 'Aggressive/Reckless Driving',\n",
    "    'disregarding yield sign': 'Aggressive/Reckless Driving',\n",
    "    'passing stopped school bus': 'Aggressive/Reckless Driving',\n",
    "    'improper overtaking/passing': 'Aggressive/Reckless Driving',\n",
    "    'failing to yield right-of-way': 'Aggressive/Reckless Driving',\n",
    "    'following too closely': 'Aggressive/Reckless Driving',\n",
    "    'improper lane usage': 'Aggressive/Reckless Driving',\n",
    "    'improper turning/no signal': 'Aggressive/Reckless Driving',\n",
    "    \n",
    "    'driving skills/knowledge/experience': 'Driver\\'s Condition/Experience',\n",
    "    'physical condition of driver': 'Driver\\'s Condition/Experience',\n",
    "    'vision obscured (signs, tree limbs, buildings, etc.)': 'Driver\\'s Condition/Experience',\n",
    "    'under the influence of alcohol/drugs (use when arrest is effected)': 'Driver\\'s Condition/Experience',\n",
    "    'had been drinking (use when arrest is not made)': 'Driver\\'s Condition/Experience',\n",
    "    \n",
    "    'weather': 'Environmental and Road Conditions',\n",
    "    'road engineering/surface/marking defects': 'Environmental and Road Conditions',\n",
    "    'road construction/maintenance': 'Environmental and Road Conditions',\n",
    "    'evasive action due to animal, object, nonmotorist': 'Environmental and Road Conditions',\n",
    "    'animal': 'Environmental and Road Conditions',\n",
    "    \n",
    "    'unable to determine': 'Unknown/Other',\n",
    "    'not applicable': 'Unknown/Other',\n",
    "    'related to bus stop': 'Unknown/Other',\n",
    "    'obstructed crosswalks': 'Unknown/Other',\n",
    "    \n",
    "    # Add the missing categories\n",
    "    'improper backing': 'Aggressive/Reckless Driving',\n",
    "    'equipment - vehicle condition': 'Driver\\'s Condition/Experience',\n",
    "    'disregarding other traffic signs': 'Aggressive/Reckless Driving',\n",
    "    'disregarding road markings': 'Aggressive/Reckless Driving',\n",
    "    'turning right on red': 'Aggressive/Reckless Driving'\n",
    "}\n",
    "\n",
    "# Apply the mapping to categorize the causes\n",
    "crashes_cleaned['crash_cause_category'] = crashes_cleaned['prim_contributory_cause'].map(cause_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d179a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Find unique values in 'prim_contributory_cause' that are not in the 'cause_mapping'\n",
    "missing_values = crashes_cleaned[~crashes_cleaned['prim_contributory_cause'].isin(cause_mapping.keys())]['prim_contributory_cause'].unique()\n",
    "\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5760440c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aggressive/Reckless Driving          417688\n",
       "Unknown/Other                        400902\n",
       "Driver's Condition/Experience         51717\n",
       "Environmental and Road Conditions     19367\n",
       "Distraction                           11772\n",
       "Name: crash_cause_category, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value counts in the new category column\n",
    "crashes_cleaned['crash_cause_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54e507b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not applicable                                                                      371652\n",
       "unable to determine                                                                 324878\n",
       "failing to reduce speed to avoid crash                                               33161\n",
       "failing to yield right-of-way                                                        28925\n",
       "driving skills/knowledge/experience                                                  28101\n",
       "following too closely                                                                23735\n",
       "improper overtaking/passing                                                          14021\n",
       "improper lane usage                                                                  12692\n",
       "weather                                                                               9915\n",
       "improper turning/no signal                                                            9382\n",
       "improper backing                                                                      7194\n",
       "operating vehicle in erratic, reckless, careless, negligent or aggressive manner      5563\n",
       "disregarding traffic signals                                                          3659\n",
       "vision obscured (signs, tree limbs, buildings, etc.)                                  2794\n",
       "physical condition of driver                                                          2724\n",
       "distraction - from inside vehicle                                                     2689\n",
       "disregarding stop sign                                                                2605\n",
       "driving on wrong side/wrong way                                                       1899\n",
       "equipment - vehicle condition                                                         1826\n",
       "exceeding authorized speed limit                                                      1473\n",
       "distraction - from outside vehicle                                                    1465\n",
       "under the influence of alcohol/drugs (use when arrest is effected)                    1459\n",
       "exceeding safe speed for conditions                                                   1438\n",
       "had been drinking (use when arrest is not made)                                       1059\n",
       "road construction/maintenance                                                         1029\n",
       "disregarding other traffic signs                                                       915\n",
       "disregarding road markings                                                             883\n",
       "road engineering/surface/marking defects                                               834\n",
       "cell phone use other than texting                                                      674\n",
       "evasive action due to animal, object, nonmotorist                                      471\n",
       "related to bus stop                                                                    436\n",
       "animal                                                                                 427\n",
       "turning right on red                                                                   350\n",
       "distraction - other electronic device (navigation device, dvd player, etc.)            255\n",
       "bicycle advancing legally on red light                                                 231\n",
       "disregarding yield sign                                                                224\n",
       "texting                                                                                159\n",
       "obstructed crosswalks                                                                   98\n",
       "passing stopped school bus                                                              93\n",
       "motorcycle advancing legally on red light                                               58\n",
       "Name: sec_contributory_cause, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within sec_contributory_cause feature\n",
    "crashes_cleaned['sec_contributory_cause'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c0965c",
   "metadata": {},
   "source": [
    "This feature is redundant to prim_contributory_cause, and a high majority of values are either 'not applicable' or 'unable to determine' so it will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "854e7e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12/29/2020 05:00:00 pm    30\n",
       "11/10/2017 10:30:00 am    27\n",
       "02/17/2022 03:30:00 pm    21\n",
       "11/21/2024 10:30:00 am    20\n",
       "11/21/2024 10:00:00 am    20\n",
       "                          ..\n",
       "12/23/2016 12:41:00 pm     1\n",
       "10/03/2020 05:32:00 pm     1\n",
       "08/02/2021 05:15:00 pm     1\n",
       "01/08/2020 02:35:00 pm     1\n",
       "09/13/2023 01:08:00 pm     1\n",
       "Name: crash_date, Length: 592919, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_date feature\n",
    "crashes_cleaned['crash_date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54343eff",
   "metadata": {},
   "source": [
    "Will remove this feature. This information is captured in crash_hour, crash_day_of_the_week, crash_month. I keep the following three features and perform reclassification to help prepare for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "528bd168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15    69825\n",
       "16    68993\n",
       "17    67144\n",
       "14    60189\n",
       "18    55381\n",
       "13    54478\n",
       "12    52818\n",
       "8     47683\n",
       "11    45742\n",
       "9     41217\n",
       "10    40942\n",
       "19    40838\n",
       "7     38207\n",
       "20    33003\n",
       "21    29440\n",
       "22    27107\n",
       "23    23508\n",
       "0     19638\n",
       "6     19488\n",
       "1     16760\n",
       "2     14336\n",
       "5     12390\n",
       "3     11848\n",
       "4     10471\n",
       "Name: crash_hour, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_hour feature\n",
    "crashes_cleaned['crash_hour'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f6e3113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    146122\n",
       "7    133158\n",
       "5    129717\n",
       "3    128456\n",
       "4    127880\n",
       "2    123620\n",
       "1    112493\n",
       "Name: crash_day_of_week, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_day_of_week feature\n",
    "crashes_cleaned['crash_day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a78ba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    86680\n",
       "9     82227\n",
       "8     80821\n",
       "7     78568\n",
       "11    78175\n",
       "6     77697\n",
       "5     77268\n",
       "12    74429\n",
       "3     67812\n",
       "4     66417\n",
       "1     66068\n",
       "2     65284\n",
       "Name: crash_month, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_month feature\n",
    "crashes_cleaned['crash_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05781f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crashes_cleaned['time_of_day'] = pd.cut(\n",
    "    crashes_cleaned['crash_hour'], \n",
    "    bins=[-1, 5, 11, 17, 23], \n",
    "    labels=['Night (Late)', 'Morning', 'Afternoon', 'Night (Early)'],\n",
    "    right=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f6a48ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned['day_of_week'] = crashes_cleaned['crash_day_of_week'].replace({\n",
    "    1: 'Sun',\n",
    "    2: 'Mon',\n",
    "    3: 'Tues',\n",
    "    4: 'Wed',\n",
    "    5: 'Thur',\n",
    "    6: 'Fri',\n",
    "    7: 'Sat'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50f4adb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fri     146122\n",
       "Sat     133158\n",
       "Thur    129717\n",
       "Tues    128456\n",
       "Wed     127880\n",
       "Mon     123620\n",
       "Sun     112493\n",
       "Name: day_of_week, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within day_of_week feature\n",
    "crashes_cleaned['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ef487d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups crash_months into seasons and saves as new feature: season\n",
    "\n",
    "crashes_cleaned['season'] = pd.cut(\n",
    "    crashes_cleaned['crash_month'], \n",
    "    bins=[0, 2, 5, 8, 11, 12], \n",
    "    labels=['Winter', 'Spring', 'Summer', 'Fall', 'Winter'],\n",
    "    right=True,\n",
    "    ordered=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a99074dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fall      247082\n",
       "Summer    237086\n",
       "Spring    211497\n",
       "Winter    205781\n",
       "Name: season, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within the new season feature\n",
    "crashes_cleaned['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ffd5b38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id                       0\n",
       "crash_date                            0\n",
       "posted_speed_limit                    0\n",
       "traffic_control_device                0\n",
       "device_condition                      0\n",
       "weather_condition                     0\n",
       "lighting_condition                    0\n",
       "first_crash_type                      0\n",
       "trafficway_type                       0\n",
       "lane_cnt                         702424\n",
       "alignment                             0\n",
       "roadway_surface_cond                  0\n",
       "road_defect                           0\n",
       "report_type                       28066\n",
       "crash_type                            0\n",
       "intersection_related_i           694392\n",
       "hit_and_run_i                    618754\n",
       "damage                                0\n",
       "date_police_notified                  0\n",
       "prim_contributory_cause               0\n",
       "sec_contributory_cause                0\n",
       "street_no                             0\n",
       "street_direction                      4\n",
       "street_name                           1\n",
       "beat_of_occurrence                    5\n",
       "num_units                             0\n",
       "most_severe_injury                 1993\n",
       "injuries_total                     1979\n",
       "injuries_fatal                     1979\n",
       "injuries_incapacitating            1979\n",
       "injuries_non_incapacitating        1979\n",
       "injuries_reported_not_evident      1979\n",
       "injuries_no_indication             1979\n",
       "injuries_unknown                   1979\n",
       "crash_hour                            0\n",
       "crash_day_of_week                     0\n",
       "crash_month                           0\n",
       "latitude                           6526\n",
       "longitude                          6526\n",
       "location                           6526\n",
       "speed_limit_category                  0\n",
       "traffic_control_category           1298\n",
       "road_category                         0\n",
       "severity_category                  1993\n",
       "crash_cause_category                  0\n",
       "time_of_day                           0\n",
       "day_of_week                           0\n",
       "season                                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the count of null values within each feature\n",
    "crashes_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0e278",
   "metadata": {},
   "source": [
    "#### 2.1.8 Remove unuseful features: `.drop()` for list of features deemed not useful for analysis and store trimmed df as ‘crashes_cleaned’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd46bcd",
   "metadata": {},
   "source": [
    "Domain knowledge and better understanding of the features and business problem led me to remove several features. Features dealing with aftermath, such as damage, and 'date_police_notified' both deal with aftermath, and thus do not offer much predictive insights, so they are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ff41ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crashes_cleaned.drop(columns = [\n",
    "    'crash_date',\n",
    "    'hit_and_run_i',\n",
    "    'device_condition',\n",
    "    'weather_condition',\n",
    "    'road_defect',\n",
    "    'crash_type',\n",
    "    'damage',\n",
    "    'date_police_notified',\n",
    "    'sec_contributory_cause',\n",
    "    'street_no',\n",
    "    'report_type',\n",
    "    'beat_of_occurrence',\n",
    "    'num_units',\n",
    "    'alignment',\n",
    "    'injuries_total',\n",
    "    'injuries_fatal',\n",
    "     'injuries_incapacitating',\n",
    "     'injuries_non_incapacitating',\n",
    "     'injuries_reported_not_evident',\n",
    "     'injuries_no_indication',\n",
    "    'injuries_unknown',\n",
    "    'location',\n",
    "    'street_direction',\n",
    "    'lane_cnt', \n",
    "    'intersection_related_i',\n",
    "    'trafficway_type', \n",
    "    'crash_hour', \n",
    "    'crash_day_of_week', \n",
    "    'crash_month', \n",
    "    'posted_speed_limit', \n",
    "    'traffic_control_device', \n",
    "    'street_name', \n",
    "    'most_severe_injury',\n",
    "    'prim_contributory_cause',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'first_crash_type'\n",
    "], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d4266c",
   "metadata": {},
   "source": [
    "#### 2.1.10 Convert data types: stored data types to reflect true data types (categorical variables as strings, numeric variables as int, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9ea3f8",
   "metadata": {},
   "source": [
    "Ensuring that features' stored data types match their true data type is another common cleaning step. Most of the features in the cleaned dataframe were categorical variables, so I saved them as 'category' data type. I could've also stored them as object types, but category types are easier to store and use less memory, which is important in contexts like this where you're dealing with such a vast amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc9c12ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id               object\n",
       "lighting_condition          category\n",
       "roadway_surface_cond        category\n",
       "speed_limit_category        category\n",
       "traffic_control_category    category\n",
       "road_category               category\n",
       "severity_category           category\n",
       "crash_cause_category        category\n",
       "time_of_day                 category\n",
       "day_of_week                 category\n",
       "season                      category\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all the columns (except 'crash_record_id') to category type\n",
    "crashes_cleaned[[col for col in crashes_cleaned.columns if col != 'crash_record_id']] = crashes_cleaned[[col for col in crashes_cleaned.columns if col != 'crash_record_id']].astype('category')\n",
    "\n",
    "# Verify the changes\n",
    "crashes_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f557732",
   "metadata": {},
   "source": [
    "Once datatypes were addressed and any unhelpful features removed, I checked the distribution of my target feature. I also had to decide how to handle the remaining null values. Since the remaining null values were such a small part of the total, I just decided to remove any row with a null value knowing that this would still leave me with plenty of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c57b7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distribution of the target variable severity_category\n",
    "crashes_cleaned['severity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dfe70",
   "metadata": {},
   "source": [
    "Checking the distribution of my target, I can see there is significant class imbalance, and this will inform some of my future data preparation decisions. Right now I still have **16**k of my target class, but I will be cautious with removing rows as I want to make sure there are enough data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e43f34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks the percentage of null values within each feature\n",
    "(crashes_cleaned.isna().sum()/ len(crashes_cleaned))* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d60046a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops any remaining rows that contain null values\n",
    "crashes_cleaned.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2521af70",
   "metadata": {},
   "source": [
    "With such small percentages of remaining null values I feel confident simply removing them as opposed to imputing values which could potentially introduce noise.\n",
    "\n",
    "The crashes_cleaned dataframe is now prepared for merging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a7db8",
   "metadata": {},
   "source": [
    "### 2.2 People "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22140b6c",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Steps:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "\n",
    "\n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (categorical variables as strings, numeric variables as int, ect.)\n",
    "\n",
    "\n",
    "* **reduce feature cardinality with label reclassification**:\n",
    "\n",
    "    * 'safety_equipment' to 'safety_equipment_category'\n",
    "    * 'age' to 'age_group'\n",
    " \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32755be9",
   "metadata": {},
   "source": [
    "#### 2.2.1 Preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "260ea367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previews the first 5 rows of the data\n",
    "people_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7e04e",
   "metadata": {},
   "source": [
    "#### 2.2.2 Understand Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "83b079e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# provides info about the dataframe\n",
    "people_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220f761",
   "metadata": {},
   "source": [
    "#### 2.2.3 Format Feature names and Row Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac980b83",
   "metadata": {},
   "source": [
    "Initial observations after previewing the first 5 rows and the dataframe structure with `.info()` are that this dataset is massive with nearly 2 million records. Extensive cleaning will need to take place to reduce the size of this dataset for computational efficiency. \n",
    "\n",
    "For a first step, I decided to make the feature names lower case simply for readability, and as a precuror cleaning step I made all string values lower case in the hopes that this may deal with some misspellings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6a74a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts all feature names to lower case\n",
    "people_df.columns = people_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af6b398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in people_df.select_dtypes(include='object').columns:\n",
    "    people_df[col] = people_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1334da0",
   "metadata": {},
   "source": [
    "#### 2.2.4 Drop features with overly high null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393f1dc",
   "metadata": {},
   "source": [
    "#### 2.1.4 Drop features with overly high null values: \n",
    "\n",
    "While there are a lot of records in this dataframe, some features also have a lot of null values.  I know that features with a significant majority of null values will not be helpful for analysis so I start by trying to identify these features and remove them right off the bat. Rather than look at the count of nulls, I used `(.isna().sum()/ len(df)) *100` to get the percentage of nulls for each feature. This is easier to grasp than null counts in the 10 and hundred thousands. \n",
    "\n",
    "For quick cleaning, I chose a 90% null threshold to automatically remove features. For the features with between less than 90% nulls, I decided to inspect them more closely before blinding removing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7491129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the percentage of null values per feature, rounded to 2 places\n",
    "round((people_df.isna().sum()/ len(people_df)*100), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c8878d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ems_agency',\n",
       " 'ems_run_no',\n",
       " 'pedpedal_action',\n",
       " 'pedpedal_visibility',\n",
       " 'pedpedal_location',\n",
       " 'bac_result value',\n",
       " 'cell_phone_use']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting all features with 90% or more of its values are null and stores features as a list\n",
    "ppl_high_null_features = list(people_df.columns[(people_df.isna().sum() / len(people_df) * 100) >= 90])\n",
    "ppl_high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1cfc6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all features with 90% or more null values\n",
    "people_cleaned = people_df.drop(columns=ppl_high_null_features)\n",
    "people_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca087a3",
   "metadata": {},
   "source": [
    "#### 2.2.5 deleting people from memory\n",
    "\n",
    "Once I created a new dataframe by dropping the features with high null counts, I no longer needed the original people_df dataset so I deleted it from memory due to its large size and ability to take up a large amount of memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "60171a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes the people_df dataframe to clear up memory\n",
    "\n",
    "del people_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df0507de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3dd7de",
   "metadata": {},
   "source": [
    "#### 2.2.6 Checking for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec9338",
   "metadata": {},
   "source": [
    "This data set has 3 different ID features, so to get a better sense of what each feature and row represent I calculated the number of rows that contain duplicates for the three id features to better understand how this dataset relates to the crashes and vehicles dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2b5db65",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the list of ID columns to check for duplicates\n",
    "id_columns = ['person_id', 'crash_record_id', 'vehicle_id']\n",
    "\n",
    "# Loop through each column to check for duplicates\n",
    "for column in id_columns:\n",
    "    # Directly calculate and print the count of duplicates\n",
    "    duplicates_count = people_cleaned[column].duplicated().sum()\n",
    "    print(f\"Number of duplicate {column} rows: {duplicates_count}\")\n",
    "\n",
    "    # If needed, display duplicate rows (uncommon for large datasets due to memory concerns)\n",
    "    if duplicates_count > 0:\n",
    "        print(f\"Example duplicate rows for {column}:\")\n",
    "        print(people_cleaned[people_cleaned[column].duplicated()].head())  # Show only the first few rows\n",
    "    print(\"\\n\" + \"=\"*50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbf6b2",
   "metadata": {},
   "source": [
    "##### Explanation of Output:\n",
    "\n",
    "1. **Duplicate person_id Rows:**\n",
    "   - No duplicate person_id rows were found, which means each person_id is unique in this dataset.\n",
    "\n",
    "2. **Duplicate crash_record_id Rows:**\n",
    "   - A total of 1,080,392 rows are duplicates based on crash_record_id. This suggests that multiple individuals (drivers, passengers, etc.) may have been associated with the same crash. This is expected if there are multiple people involved in the same crash event.\n",
    "\n",
    "3. **Duplicate vehicle_id Rows:**\n",
    "   - A total of 421,011 rows are duplicates based on vehicle_id. This indicates that some vehicles appear in multiple records, potentially due to different passengers or crashes involving the same vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d6daf",
   "metadata": {},
   "source": [
    "#### 2.2.7 Inspect remaining features\n",
    "\n",
    "Once columns with high null values were taken care of, I used `.value_counts()` to inspect many of the remaining columns to look for things like cardianlity, misspelling, class labels, stored data types, etc. I used some domain knowledge to avoid looking through every single feature. Similar to my process in the traffic_crashes dataframe, some for useful features with potentially unclear labels or a high cardinality, reclassify feature labels into less, more easily understandable labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4bf9eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['person_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "27b3cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['crash_date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04a5eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['seat_no'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f59bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['safety_equipment'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4254108",
   "metadata": {},
   "source": [
    "Could be an important predictor, but need to reduce cardinality. In the cell below, I reclassify the `safety_equipment` labels into 4 interpretable classes. This reclassification helps reduce cardinality which will be helpful during OneHotEncoding and improve computation efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eae62b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map the original 'safety_equipment' values to broader categories\n",
    "safety_equipment_mapping = {\n",
    "    # Used Equipment\n",
    "    'safety belt used': 'Used',\n",
    "    'child restraint used': 'Used',\n",
    "    'child restraint - forward facing': 'Used',\n",
    "    'bicycle helmet (pedacyclist involved only)': 'Used',\n",
    "    'child restraint - type unknown': 'Used',\n",
    "    'child restraint - rear facing': 'Used',\n",
    "    'dot compliant motorcycle helmet': 'Used',\n",
    "    'helmet used': 'Used',\n",
    "    'booster seat': 'Used',\n",
    "    'child restraint used improperly': 'Used',\n",
    "\n",
    "    # Not Used Equipment\n",
    "    'safety belt not used': 'Not Used',\n",
    "    'helmet not used': 'Not Used',\n",
    "    'child restraint not used': 'Not Used',\n",
    "    'not dot compliant motorcycle helmet': 'Not Used',\n",
    "    'should/lap belt used improperly': 'Not Used',\n",
    "\n",
    "    # Unknown Equipment Usage\n",
    "    'usage unknown': 'Unknown',\n",
    "\n",
    "    # Other/Special Case Equipment\n",
    "    'none present': 'Other/Special Case', \n",
    "    'wheelchair': 'Other/Special Case',\n",
    "    'stretcher': 'Other/Special Case',\n",
    "    \n",
    "    # Catch-all for any unknown or missing values\n",
    "    'unknown': 'Other/Special Case',  \n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'safety_equipment' column\n",
    "people_cleaned['safety_equipment_category'] = people_cleaned['safety_equipment'].map(safety_equipment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3a86e74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Used                  0.478260\n",
       "Unknown               0.476501\n",
       "Other/Special Case    0.033962\n",
       "Not Used              0.011277\n",
       "Name: safety_equipment_category, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the value counts for the new grouped categories\n",
    "people_cleaned['safety_equipment_category'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d4f5f",
   "metadata": {},
   "source": [
    "Based on this output, even after recategorization, this feature will not be very useful. About 95% of the data is split between \"used\" and \"unknown\", with the remaining 5% split between \"other/special case\" and \"not used\". I will drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3db5f1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "did not deploy                            987711\n",
       "not applicable                            424194\n",
       "deployment unknown                        397461\n",
       "deployed, front                            61565\n",
       "deployed, combination                      50895\n",
       "deployed, side                             17944\n",
       "deployed other (knee, air, belt, etc.)       973\n",
       "Name: airbag_deployed, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['airbag_deployed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1433359",
   "metadata": {},
   "source": [
    "This feature might be more helpful if we simply knew: did the airbag deploy or not? In the cell below, I reclassify the `airbag_deployed` labels into 3 interpretable classes. This reclassification helps reduce cardinality which will be helpful during OneHotEncoding and improve computation efficiency.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf068c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for airbag_deployed\n",
    "airbag_mapping = {\n",
    "    'did not deploy': 'Not Deployed',\n",
    "    'not applicable': 'Not Deployed',  # Assuming \"not applicable\" should be considered as unknown\n",
    "    'deployment unknown': 'Unknown',\n",
    "    'deployed, front': 'Deployed',\n",
    "    'deployed, combination': 'Deployed',\n",
    "    'deployed, side': 'Deployed',\n",
    "    'deployed other (knee, air, belt, etc.)': 'Deployed'\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'airbag_deployed' column\n",
    "people_cleaned['airbag_deployed'] = people_cleaned['airbag_deployed'].map(airbag_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a3e4f447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Not Deployed    1411905\n",
       "Unknown          397461\n",
       "Deployed         131377\n",
       "Name: airbag_deployed, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the new feature's value counts\n",
    "people_cleaned['airbag_deployed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "57727866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                  1820806\n",
       "unknown                125622\n",
       "totally ejected          5904\n",
       "partially ejected        1449\n",
       "trapped/extricated       1196\n",
       "Name: ejection, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['ejection'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025de437",
   "metadata": {},
   "source": [
    "This feature might be normally be helpful, but it is far too skewed to be helpful for this analysis. It contains about 8.5k values other than 'none' or 'unknown'. This will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9a5f8c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no indication of injury     1803602\n",
       "nonincapacitating injury      98271\n",
       "reported, not evident         58267\n",
       "incapacitating injury         17878\n",
       "fatal                          1089\n",
       "Name: injury_classification, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['injury_classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d3d7dd",
   "metadata": {},
   "source": [
    "We can drop this. It contains similar information to 'most_severe_injury' but is more imbalanced so I will drop it and keep 'most_severe_injury' as my target. This decision is aided by the fact that for this project I am focused more on crash-level data, than people-level data. Simply, I want to focus on which crashes resulted in serious injury, more general than which people were seriously injured. So while I could compute this with injury_classification, most_severe_injury already describes what I'm analyzing and thus will require less target feature preparation than if I chose to use injury_classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "32176ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                                 0.36\n",
       "unknown                              0.25\n",
       "failed to yield                      0.09\n",
       "other                                0.09\n",
       "followed too closely                 0.06\n",
       "improper backing                     0.03\n",
       "improper turn                        0.03\n",
       "improper lane change                 0.03\n",
       "improper passing                     0.02\n",
       "disregarded control devices          0.02\n",
       "too fast for conditions              0.01\n",
       "wrong way/side                       0.00\n",
       "improper parking                     0.00\n",
       "overcorrected                        0.00\n",
       "evading police vehicle               0.00\n",
       "cell phone use other than texting    0.00\n",
       "emergency vehicle on call            0.00\n",
       "texting                              0.00\n",
       "stopped school bus                   0.00\n",
       "license restrictions                 0.00\n",
       "Name: driver_action, dtype: float64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints class distribution percentages of driver_action, rounded to 2 places\n",
    "round(people_cleaned['driver_action'].value_counts(normalize = True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ab937",
   "metadata": {},
   "source": [
    "This feature is similar to 'prim_contributory_cause' in crashes, but this one contains 20% nulls. Will drop this and keep prim_contributory_cause instead to avoid multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9cbba650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not obscured              784184\n",
       "unknown                   753832\n",
       "other                      15389\n",
       "moving vehicles             8800\n",
       "parked vehicles             5429\n",
       "windshield (water/ice)      4169\n",
       "blinded - sunlight          1879\n",
       "trees, plants                614\n",
       "buildings                    558\n",
       "blinded - headlights         168\n",
       "blowing materials            108\n",
       "hillcrest                    102\n",
       "embankment                    85\n",
       "signboard                     38\n",
       "Name: driver_vision, dtype: int64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['driver_vision'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66678171",
   "metadata": {},
   "source": [
    "This feature is too skewed to provide any real analytical benefit. The top two classes by 500k values are 'not_obscured' and 'unknown', so I will drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad7b2eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "normal                          1020511\n",
       "unknown                          527441\n",
       "impaired - alcohol                 6635\n",
       "removed by ems                     5697\n",
       "other                              4585\n",
       "emotional                          4213\n",
       "fatigued/asleep                    4108\n",
       "illness/fainted                    1408\n",
       "had been drinking                  1128\n",
       "impaired - drugs                    726\n",
       "impaired - alcohol and drugs        416\n",
       "medicated                           193\n",
       "Name: physical_condition, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['physical_condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e293f3",
   "metadata": {},
   "source": [
    "Most classes here are 'normal' or 'unknown' which is not particularly insightful, so I will remove this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cbc5802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test not offered                   1554246\n",
       "test refused                         16163\n",
       "test performed, results unknown       3715\n",
       "test taken                            2784\n",
       "Name: bac_result, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['bac_result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c39ea",
   "metadata": {},
   "source": [
    "Most values are 'test not offered' and 'test refused'. Again, not very insightful, so I will drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5120b810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m    1023595\n",
       "f     743117\n",
       "x     179808\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac07487",
   "metadata": {},
   "source": [
    "This feature could be insightful. Will change the label names to be more descriptive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e598dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'other', 'female', nan], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the values in the 'sex' feature\n",
    "people_cleaned['sex'] = people_cleaned['sex'].replace({'m': 'male', 'f': 'female', 'x': 'other'})\n",
    "\n",
    "# Verify the changes\n",
    "people_cleaned['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e1719d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 26.0     39239\n",
       " 25.0     39229\n",
       " 27.0     39187\n",
       " 28.0     38579\n",
       " 24.0     38031\n",
       "          ...  \n",
       "-40.0         1\n",
       "-177.0        1\n",
       "-49.0         1\n",
       "-47.0         1\n",
       "-59.0         1\n",
       "Name: age, Length: 117, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints the distribution of values within crash_type feature\n",
    "people_cleaned['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1c314f",
   "metadata": {},
   "source": [
    "This information could be interesting to investigate and feels like it could be an insightful predictor, but its high cardinality means it will need some reclassifying in order to best be insightful. I chose to reclassify this based on age group, focusing on delineating between ages too young to drive **<16** years old, ages that are eligible to drive but brains are not fully developed (**16-26**), ages of fully developed adult brains (**27-65**), and then older folks (**65+**). I am hopeful that these labels will provide more insight than the original labels and help improve computational efficiency during modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3b27ca1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age age_group\n",
      "0  25.0     16-26\n",
      "1  37.0     27-65\n",
      "2   NaN       NaN\n",
      "3   NaN       NaN\n",
      "4   NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Sample data (replace with your actual DataFrame)\n",
    "age_bins_df = pd.DataFrame({\n",
    "    'age': [5, 15, 16, 25, 30, 60, 100, 120, -5, 200]\n",
    "})\n",
    "\n",
    "# Define the bins for age groups\n",
    "age_bins = [1, 16, 27, 66, 115]\n",
    "\n",
    "# Labels for the age groups\n",
    "age_labels = ['1-15', '16-26', '27-65', '65+']  \n",
    "\n",
    "# Apply corrections for age values outside the valid range (negative, 0, or greater than 115)\n",
    "people_cleaned['age'] = people_cleaned['age'].apply(lambda x: np.nan if x < 1 or x > 115 else x)\n",
    "\n",
    "# Apply pd.cut() to create a new 'age_group' column\n",
    "people_cleaned['age_group'] = pd.cut(people_cleaned['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "# Print the first few rows to verify the new grouping\n",
    "print(people_cleaned[['age', 'age_group']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981eccf7",
   "metadata": {},
   "source": [
    "#### 2.2.8 remove unuseful features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad28c1",
   "metadata": {},
   "source": [
    "Using domain knowledge and understanding of the business problem and target varaible, I decided several features such as drivers license state and class are not relevant, so I dropped them. After all was said and done with feature removal, the remaining dataset had 4 features, one of which was the secondary key: crash_record_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "275086b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1979859 entries, 0 to 1979858\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Dtype   \n",
      "---  ------           -----   \n",
      " 0   crash_record_id  object  \n",
      " 1   sex              object  \n",
      " 2   airbag_deployed  object  \n",
      " 3   age_group        category\n",
      "dtypes: category(1), object(3)\n",
      "memory usage: 47.2+ MB\n"
     ]
    }
   ],
   "source": [
    "people_cleaned.drop(columns=['person_id',\n",
    "                      'person_type',\n",
    "                      'vehicle_id',\n",
    "                      'drivers_license_state', \n",
    "                      'drivers_license_class',\n",
    "                      'city', \n",
    "                      'state', \n",
    "                      'zipcode',\n",
    "                      'hospital', \n",
    "                      'crash_date',\n",
    "                      'seat_no',\n",
    "                      'ejection',\n",
    "                      'injury_classification',\n",
    "                      'driver_vision',\n",
    "                      'driver_action',\n",
    "                      'physical_condition',\n",
    "                      'bac_result',\n",
    "                      'age', \n",
    "                      'safety_equipment',\n",
    "                      'safety_equipment_category'\n",
    "                     ], inplace = True)\n",
    "people_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5e4f96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the specified columns to category type\n",
    "people_cleaned[['age_group', 'airbag_deployed', 'sex']] = people_cleaned[['age_group', 'airbag_deployed', 'sex']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ac06a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the percentages of null values within each feature\n",
    "(people_cleaned.isna().sum()/ len(people_cleaned)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8537d2e2",
   "metadata": {},
   "source": [
    "By printing the percentage of null values in the remaining columns I see that sex and airbag_deployed only have a small percentage of null values, while age_group has almost 15 times more. Despite this, age_group still contains a majority of non-null values. With the dataset now containing nearly 2 million records, I feel fine simply dropping all rows with null values, as I will still have plenty of data to work with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "454805ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drops all rows with null values\n",
    "people_cleaned.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7b5d1702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirms that cleaning of this dataset is complete\n",
    "people_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a7cab4",
   "metadata": {},
   "source": [
    "With zero null values in each of 4 features, next up I will need to aggregate this data to help with merging the three datasets. I will provide further justifications for aggregations and my merging process further along in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37718f95",
   "metadata": {},
   "source": [
    "### 2.3 Vehicles\n",
    "\n",
    "2.3.4 Drop features with overly high null values\n",
    "\n",
    "2.3.5 Check for duplicates\n",
    "\n",
    "2.3.6 inspect remaining features\n",
    "\n",
    "2.3.6.1 reduce feature cardinality with bucketing\n",
    "\n",
    "2.3.7 remove unuseful features\n",
    "\n",
    "2.3.8 Convert data types\n",
    "\n",
    "2.3.9 remove remaining nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2682efd",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* Steps:\n",
    "* **Preview Data**: `.head()`\n",
    "\n",
    "\n",
    "* **Understand Structure**: `.info()`\n",
    "\n",
    "\n",
    "* **Format Feature names and Row Values**: `.lower()`\n",
    "\n",
    "\n",
    "* **Drop features with overly high null values**: `.isna().sum()/ len(df)` for percentage of nulls for each feature\n",
    "\n",
    "\n",
    "* **Check for duplicates**: `.duplicated().sum()`\n",
    "\n",
    "\n",
    "* **inspect remaining features**: `.value_counts()`; \n",
    "\n",
    "    * make intentional decisions to keep or drop using `.value_counts()` distribution and domain knowledge; \n",
    "    * make note of any features to keep that will need cleaning/cardinality reduction/etc.\n",
    "\n",
    "\n",
    "* **remove unuseful features**: \n",
    "\n",
    "    * `.drop()` for list of features deemed not useful for analysis; \n",
    "    * store trimmed df as 'df_name_cleaned'\n",
    "\n",
    "\n",
    "* **reduce feature cardinality with label reclassification**:\n",
    "\n",
    "    * 'safety_equipment' to 'safety_equipment_category'\n",
    "    * 'age' to 'age_group'\n",
    " \n",
    " \n",
    "* **Convert data types**: \n",
    "\n",
    "    * stored data types to reflect true data types \n",
    "    * (text as string, categorical variables as categories, numeric variables as int, ect.) \n",
    " \n",
    "* **remove remaining nulls**: `.dropna()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf833e37",
   "metadata": {},
   "source": [
    "#### 2.3.1 Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "96d0d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5454859",
   "metadata": {},
   "source": [
    "#### 2.3.2 Understand Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69a03d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcc09c1",
   "metadata": {},
   "source": [
    "#### 2.3.3 Format Feature names and Row Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7474fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_df.columns = vehicles_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae81a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in vehicles_df.select_dtypes(include='object').columns:\n",
    "    vehicles_df[col] = vehicles_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e959f56",
   "metadata": {},
   "source": [
    "#### 2.3.4 Drop features with overly high null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f81746ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmrc_veh_i',\n",
       " 'fire_i',\n",
       " 'exceed_speed_limit_i',\n",
       " 'towed_by',\n",
       " 'towed_to',\n",
       " 'area_00_i',\n",
       " 'area_03_i',\n",
       " 'area_04_i',\n",
       " 'area_09_i',\n",
       " 'cmv_id',\n",
       " 'usdot_no',\n",
       " 'ccmc_no',\n",
       " 'ilcc_no',\n",
       " 'commercial_src',\n",
       " 'gvwr',\n",
       " 'carrier_name',\n",
       " 'carrier_state',\n",
       " 'carrier_city',\n",
       " 'hazmat_placards_i',\n",
       " 'hazmat_name',\n",
       " 'un_no',\n",
       " 'hazmat_present_i',\n",
       " 'hazmat_report_i',\n",
       " 'hazmat_report_no',\n",
       " 'mcs_report_i',\n",
       " 'mcs_report_no',\n",
       " 'hazmat_vio_cause_crash_i',\n",
       " 'mcs_vio_cause_crash_i',\n",
       " 'idot_permit_no',\n",
       " 'wide_load_i',\n",
       " 'trailer1_width',\n",
       " 'trailer2_width',\n",
       " 'trailer1_length',\n",
       " 'trailer2_length',\n",
       " 'total_vehicle_length',\n",
       " 'axle_cnt',\n",
       " 'vehicle_config',\n",
       " 'cargo_body_type',\n",
       " 'load_type',\n",
       " 'hazmat_out_of_service_i',\n",
       " 'mcs_out_of_service_i',\n",
       " 'hazmat_class']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a list of all features with 90% or more of its values are null\n",
    "high_null_features = list(vehicles_df.columns[(vehicles_df.isna().sum() / len(vehicles_df) * 100) >= 90])\n",
    "high_null_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53424a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned = vehicles_df.drop(columns=high_null_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22084a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_unit_id</th>\n",
       "      <th>crash_record_id</th>\n",
       "      <th>crash_date</th>\n",
       "      <th>unit_no</th>\n",
       "      <th>unit_type</th>\n",
       "      <th>num_passengers</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>lic_plate_state</th>\n",
       "      <th>...</th>\n",
       "      <th>area_02_i</th>\n",
       "      <th>area_05_i</th>\n",
       "      <th>area_06_i</th>\n",
       "      <th>area_07_i</th>\n",
       "      <th>area_08_i</th>\n",
       "      <th>area_10_i</th>\n",
       "      <th>area_11_i</th>\n",
       "      <th>area_12_i</th>\n",
       "      <th>area_99_i</th>\n",
       "      <th>first_contact_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [crash_unit_id, crash_record_id, crash_date, unit_no, unit_type, num_passengers, vehicle_id, make, model, lic_plate_state, vehicle_year, vehicle_defect, vehicle_type, vehicle_use, travel_direction, maneuver, towed_i, occupant_cnt, area_01_i, area_02_i, area_05_i, area_06_i, area_07_i, area_08_i, area_10_i, area_11_i, area_12_i, area_99_i, first_contact_point]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicates based on crash_unit_id, crash_record_id, and vehicle_id\n",
    "vehicles_cleaned[vehicles_cleaned.duplicated(subset=['crash_unit_id', 'crash_record_id', 'vehicle_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a3e44",
   "metadata": {},
   "source": [
    "The output tells us that there are no duplicate rows in the vehicles_cleaned DataFrame based on the specified subset of columns: crash_unit_id, crash_record_id, and vehicle_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d0d9b",
   "metadata": {},
   "source": [
    "#### 2.3.5 deleting vehicles dataset from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce1a48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes the vehicle_df dataframe to clear up memory\n",
    "\n",
    "del vehicles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ed30962f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f619c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['num_passengers'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12ca68e",
   "metadata": {},
   "source": [
    "This is redundant information. This information does not include the driver, but this information is captured in occupant count. will drop this one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1ec7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['unit_no'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19edb3c",
   "metadata": {},
   "source": [
    "This is aftermath. unhelpful. remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6d26a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicles_cleaned['unit_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c629bd",
   "metadata": {},
   "source": [
    "Most of the values are drivers or parked cars. this will not be useful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ed2c211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vehicles_cleaned['make'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae12dad3",
   "metadata": {},
   "source": [
    "high cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35aa60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['model'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e6113",
   "metadata": {},
   "source": [
    "This feels like it could be helpful, but many unknowns and 'other', and very high cardinality. The important information that we'd gain from this is already included in vehicle_type. So we can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0d3d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['vehicle_defect'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776a4117",
   "metadata": {},
   "source": [
    "Most of the values are none or unknown. This will not be particularly useful for analysis. can drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db954d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['vehicle_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ea7543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['travel_direction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18904ac8",
   "metadata": {},
   "source": [
    "Unhelpful for analysis. Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "47603a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['maneuver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802fd35",
   "metadata": {},
   "source": [
    "This feature could be important as it has to do with what was happening prior to the crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf957d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['towed_i'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46f2b24",
   "metadata": {},
   "source": [
    "Aftermath; Unhelpful for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d9ada0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['occupant_cnt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f393d2f6",
   "metadata": {},
   "source": [
    "It is unclear what the area_##_i features represent. They will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c1a85b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['first_contact_point'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed003522",
   "metadata": {},
   "source": [
    "This feature could indicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f218c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_features_to_drop = ['num_passengers', \n",
    "                            'crash_unit_id',\n",
    "                            'crash_date',\n",
    "                            'unit_type',\n",
    "                            'make', \n",
    "                            'model',\n",
    "                            'vehicle_id',\n",
    "                           'vehicle_defect',\n",
    "                           'unit_no',\n",
    "                           'lic_plate_state',\n",
    "                            'vehicle_year',\n",
    "                           'vehicle_use',\n",
    "                           'travel_direction',\n",
    "                           'towed_i',\n",
    "                            'area_01_i',\n",
    "                           'area_02_i', \n",
    "                            'area_05_i',\n",
    "                            'area_06_i',\n",
    "                            'area_07_i',\n",
    "                            'area_08_i',\n",
    "                            'area_10_i',\n",
    "                            'area_11_i',\n",
    "                            'area_12_i',\n",
    "                            'area_99_i', \n",
    "                           'first_contact_point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0a275b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned = vehicles_cleaned.drop(columns=vehicle_features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "34dfd8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b6cb91c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crash_record_id', 'vehicle_type', 'maneuver', 'occupant_cnt']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vehicles_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "49467321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Passenger Vehicles                   1210579\n",
       "SUVs                                  250398\n",
       "Trucks                                114924\n",
       "Buses                                  25067\n",
       "Other                                  24559\n",
       "Motorcycles                             6140\n",
       "Recreational/Off-Highway Vehicles        237\n",
       "Farm and Specialized Equipment            87\n",
       "Name: vehicle_category, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map the original vehicle types to more specific categories\n",
    "vehicle_type_mapping = {\n",
    "    'passenger': 'Passenger Vehicles',\n",
    "    'sport utility vehicle (suv)': 'SUVs',\n",
    "    'van/mini-van': 'Passenger Vehicles',\n",
    "    'pickup': 'Trucks',\n",
    "    'truck - single unit': 'Trucks',\n",
    "    'single unit truck with trailer': 'Trucks',\n",
    "    'other': 'Other',\n",
    "    'bus over 15 pass.': 'Buses',\n",
    "    'bus up to 15 pass.': 'Buses',\n",
    "    'tractor w/ semi-trailer': 'Trucks',\n",
    "    'tractor w/o semi-trailer': 'Trucks',\n",
    "    'motorcycle (over 150cc)': 'Motorcycles',\n",
    "    'other vehicle with trailer': 'Other',\n",
    "    'autocycle': 'Motorcycles',\n",
    "    'moped or motorized bicycle': 'Motorcycles',\n",
    "    'motor driven cycle': 'Motorcycles',\n",
    "    'all-terrain vehicle (atv)': 'Recreational/Off-Highway Vehicles',\n",
    "    'farm equipment': 'Farm and Specialized Equipment',\n",
    "    '3-wheeled motorcycle (2 rear wheels)': 'Motorcycles',\n",
    "    'recreational off-highway vehicle (rov)': 'Recreational/Off-Highway Vehicles',\n",
    "    'snowmobile': 'Recreational/Off-Highway Vehicles',\n",
    "    'unknown/na': np.nan  # Set 'unknown/na' to NaN\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'vehicle_type' column\n",
    "vehicles_cleaned['vehicle_category'] = vehicles_cleaned['vehicle_type'].map(vehicle_type_mapping)\n",
    "\n",
    "# Check the value counts for the new grouped categories\n",
    "vehicles_cleaned['vehicle_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4656fba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Passenger Vehicles    1210579\n",
       "SUVs                   250398\n",
       "Trucks                 114924\n",
       "Buses                   25067\n",
       "Other                   24559\n",
       "Motorcycles              6140\n",
       "Name: vehicle_category, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows with 'Recreational/Off-Highway Vehicles' and 'Farm and Specialized Equipment'\n",
    "vehicles_cleaned = vehicles_cleaned[~vehicles_cleaned['vehicle_category'].isin(['Recreational/Off-Highway Vehicles', 'Farm and Specialized Equipment'])]\n",
    "\n",
    "# Check the value counts after removing those categories\n",
    "vehicles_cleaned['vehicle_category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "40ff9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "46fa0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the maneuver mapping to treat 'unknown/na' as NaN\n",
    "maneuver_mapping = {\n",
    "    'straight ahead': 'Standard Movement',\n",
    "    'slow/stop in traffic': 'Standard Movement',\n",
    "    'passing/overtaking': 'Standard Movement',\n",
    "    'unknown/na': np.nan,  # Set 'unknown/na' to NaN\n",
    "    \n",
    "    'parked': 'Reversing/Stopping',\n",
    "    'entering traffic lane from parking': 'Reversing/Stopping',\n",
    "    'starting in traffic': 'Reversing/Stopping',\n",
    "    \n",
    "    'turning left': 'Turn/Change of Direction',\n",
    "    'turning right': 'Turn/Change of Direction',\n",
    "    'u-turn': 'Turn/Change of Direction',\n",
    "    'changing lanes': 'Turn/Change of Direction',\n",
    "    'turning on red': 'Turn/Change of Direction',\n",
    "    \n",
    "    'backing': 'Reversing/Stopping',\n",
    "    'avoiding vehicles/objects': 'Avoidance/Emergency Response',\n",
    "    'skidding/control loss': 'Avoidance/Emergency Response',\n",
    "    'negotiating a curve': 'Avoidance/Emergency Response',\n",
    "    \n",
    "    'leaving traffic lane to park': 'Reversing/Stopping',\n",
    "    'enter from drive/alley': 'Reversing/Stopping',\n",
    "    \n",
    "    'driving wrong way': 'Special Cases',\n",
    "    'diverging': 'Special Cases',\n",
    "    'driverless': 'Special Cases',\n",
    "    'disabled': 'Special Cases',\n",
    "    \n",
    "    'other': 'Special Cases',\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'maneuver' column\n",
    "vehicles_cleaned['maneuver_category'] = vehicles_cleaned['maneuver'].map(maneuver_mapping)\n",
    "\n",
    "# Check the value counts for the new 'maneuver_category'\n",
    "vehicles_cleaned['maneuver_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4cf3bf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Single Occupancy    248056\n",
       "Small Group           8763\n",
       "Medium Group           429\n",
       "Large Group             91\n",
       "Very Large Group         0\n",
       "Name: occupant_category, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 0, 99, and negative values with NaN in 'occupant_cnt' for future dropping\n",
    "vehicles_cleaned['occupant_cnt'] = vehicles_cleaned['occupant_cnt'].replace([0, 99], np.nan)\n",
    "\n",
    "# Define bins for the categories (including the 20-98 range for Very Large Group)\n",
    "bins = [1, 4, 8, 19, 98, float('inf')]  # Adjusted to include 20-98 for Very Large Group\n",
    "labels = ['Single Occupancy', 'Small Group', 'Medium Group', 'Large Group', 'Very Large Group']  # 5 labels for 5 bins\n",
    "\n",
    "# Use pd.cut to categorize the 'occupant_cnt' column based on the bins\n",
    "vehicles_cleaned['occupant_category'] = pd.cut(\n",
    "    vehicles_cleaned['occupant_cnt'], \n",
    "    bins=bins, \n",
    "    labels=labels, \n",
    "    right=True, \n",
    "    include_lowest=False  # Exclude 0 from Single Occupancy\n",
    ")\n",
    "\n",
    "# Handle the NaN values for 'occupant_category' (those rows with invalid occupant_cnt, such as 0 or 99)\n",
    "vehicles_cleaned['occupant_category'] = vehicles_cleaned['occupant_category'].where(\n",
    "    vehicles_cleaned['occupant_category'].notna(), np.nan\n",
    ")\n",
    "\n",
    "# Check the categories to ensure the correct label reclassification\n",
    "vehicles_cleaned['occupant_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2fe203aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Single Occupancy    248056\n",
       "Small Group           8763\n",
       "Medium Group           429\n",
       "Large Group             91\n",
       "Name: occupant_category, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove 'Very Large Group' rows since it contains zero values\n",
    "vehicles_cleaned = vehicles_cleaned[vehicles_cleaned['occupant_category'] != 'Very Large Group']\n",
    "\n",
    "# Drop 'Very Large Group' from the categorical data if it still exists\n",
    "vehicles_cleaned['occupant_category'] = vehicles_cleaned['occupant_category'].cat.remove_categories('Very Large Group')\n",
    "\n",
    "# Check the categories to ensure the correct label reclassification\n",
    "vehicles_cleaned['occupant_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "017fa4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "40c3c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a2db7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['occupant_cnt'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c11e49b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned['occupant_cnt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1402c406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'occupant_category' and 'occupant_cnt' column\n",
    "vehicles_cleaned = vehicles_cleaned.drop(columns=['occupant_category', 'occupant_cnt', 'vehicle_type', 'maneuver'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e98afa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id           0\n",
       "vehicle_category     206831\n",
       "maneuver_category    204227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7b7d7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any null (NaN) values\n",
    "vehicles_cleaned = vehicles_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f7501b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a99441da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef3b2f",
   "metadata": {},
   "source": [
    "### Merging `crashes_cleaned`, `people_cleaned`, & `vehicles_cleaned`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25920e12",
   "metadata": {},
   "source": [
    "Relationships Between Tables and Justification for Merging\n",
    "\n",
    "1. Relationship Between crashes_cleaned and people_cleaned\n",
    "\t•\tcrash_record_id is the primary key in crashes_cleaned and appears in people_cleaned.\n",
    "\t•\tEach crash in crashes_cleaned can involve multiple people (drivers, passengers, pedestrians).\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tOne-to-Many: One crash (crashes_cleaned) can have many people (people_cleaned) associated with it.\n",
    "\n",
    "2. Relationship Between crashes_cleaned and vehicles_cleaned\n",
    "\t•\tcrash_record_id is the primary key in crashes_cleaned and appears in vehicles_cleaned.\n",
    "\t•\tEach crash in crashes_cleaned can involve multiple vehicles.\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tOne-to-Many: One crash (crashes_cleaned) can have many vehicles (vehicles_cleaned) associated with it.\n",
    "\n",
    "3. Relationship Between people_cleaned and vehicles_cleaned\n",
    "\t•\tBoth tables are linked via crash_record_id, but they describe different entities.\n",
    "\t•\tPeople (people_cleaned) and vehicles (vehicles_cleaned) may not have a direct relationship unless there’s another shared identifier (e.g., vehicle_id or person_id).\n",
    "\n",
    "This means the relationship is:\n",
    "\t•\tMany-to-Many: Many people can be in many vehicles within the same crash. (However, this relationship is indirectly expressed through crash_record_id.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cba42",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daed44c",
   "metadata": {},
   "source": [
    "The goal of aggregating the people_cleaned dataset is to handle the many-to-one relationship between the people_cleaned and crashes_cleaned datasets. Each crash_record_id in crashes_cleaned may have multiple associated records in people_cleaned, as a single crash may involve multiple people. Since our focus is on predicting the severity of crashes using the severity_category (the target variable in crashes_cleaned), we need to aggregate the people data to ensure each crash record has only one corresponding row of data.\n",
    "\n",
    "The aggregation process involves grouping the people_cleaned dataset by crash_record_id, which is the shared key between the two datasets. For features like sex, age_group, and airbag_deployed, we use the most frequent value for each crash. In cases where there is a tie (e.g., multiple values with the same frequency), we resolve the tie by selecting the value with the highest count using the idxmax() function on the value counts of each group. This ensures consistency and avoids ambiguity in cases of a tie.\n",
    "\n",
    "Similarly, the vehicles_cleaned dataset also has a many-to-one relationship with crashes_cleaned, where each crash_record_id in crashes_cleaned can have multiple associated vehicle records. As with the people_cleaned data, we need to aggregate the vehicle data to ensure that each crash record has a corresponding single row. The aggregation will allow us to focus on features like vehicle_category and other vehicle-specific attributes that might affect crash severity.\n",
    "\n",
    "By grouping the vehicles_cleaned dataset by crash_record_id, we can apply the same aggregation logic as with the people data. This ensures that we retain the most important vehicle-specific features while also maintaining a consistent one-to-one relationship between crashes_cleaned and the aggregated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "837c4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 298.16 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_record_id</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>airbag_deployed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000013b0123279411e0ec856dae95ab9f0851764350b7f...</td>\n",
       "      <td>female</td>\n",
       "      <td>27-65</td>\n",
       "      <td>Not Deployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002c0771fb6f2c70ba775b7f6b501608cadea85c1dd1...</td>\n",
       "      <td>female</td>\n",
       "      <td>27-65</td>\n",
       "      <td>Not Deployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00005696946846c8b8a1d378dba4e2a5ed84a9b2876fe0...</td>\n",
       "      <td>male</td>\n",
       "      <td>27-65</td>\n",
       "      <td>Not Deployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000070ed7a6357c3298f5edc6fb7d5ce925a10f46660f3...</td>\n",
       "      <td>male</td>\n",
       "      <td>27-65</td>\n",
       "      <td>Not Deployed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000c280b9c15e9ec96aa2eed34bf0f3ef1d604c6ea460...</td>\n",
       "      <td>female</td>\n",
       "      <td>27-65</td>\n",
       "      <td>Not Deployed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     crash_record_id     sex age_group  \\\n",
       "0  000013b0123279411e0ec856dae95ab9f0851764350b7f...  female     27-65   \n",
       "1  00002c0771fb6f2c70ba775b7f6b501608cadea85c1dd1...  female     27-65   \n",
       "2  00005696946846c8b8a1d378dba4e2a5ed84a9b2876fe0...    male     27-65   \n",
       "3  000070ed7a6357c3298f5edc6fb7d5ce925a10f46660f3...    male     27-65   \n",
       "4  0000c280b9c15e9ec96aa2eed34bf0f3ef1d604c6ea460...  female     27-65   \n",
       "\n",
       "  airbag_deployed  \n",
       "0    Not Deployed  \n",
       "1    Not Deployed  \n",
       "2    Not Deployed  \n",
       "3    Not Deployed  \n",
       "4    Not Deployed  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform a single groupby and aggregate all columns at once\n",
    "people_aggregated = (\n",
    "    people_cleaned.groupby('crash_record_id').agg({\n",
    "        'sex': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan,\n",
    "        'age_group': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan,\n",
    "        'airbag_deployed': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "    }).reset_index()\n",
    ")\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Previews the aggregated results\n",
    "people_aggregated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b037c",
   "metadata": {},
   "source": [
    "Large Data Processing:\n",
    "\n",
    "* If you’re working with large datasets (e.g., in data science or machine learning) and have explicitly deleted variables (e.g., using del) but still notice high memory usage, calling gc.collect() ensures the unused memory is released.\n",
    "    \n",
    "Long-Running Programs:\n",
    "\n",
    "* For applications that run continuously (like servers or data pipelines), invoking gc.collect() at specific intervals can help manage memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2acdef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deletes the people_cleaned dataframe to clear up memory\n",
    "del people_cleaned\n",
    "\n",
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1be7fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 266.49 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_record_id</th>\n",
       "      <th>vehicle_category</th>\n",
       "      <th>maneuver_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000013b0123279411e0ec856dae95ab9f0851764350b7f...</td>\n",
       "      <td>Passenger Vehicles</td>\n",
       "      <td>Reversing/Stopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00002c0771fb6f2c70ba775b7f6b501608cadea85c1dd1...</td>\n",
       "      <td>Passenger Vehicles</td>\n",
       "      <td>Standard Movement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00005696946846c8b8a1d378dba4e2a5ed84a9b2876fe0...</td>\n",
       "      <td>Trucks</td>\n",
       "      <td>Reversing/Stopping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000070ed7a6357c3298f5edc6fb7d5ce925a10f46660f3...</td>\n",
       "      <td>Passenger Vehicles</td>\n",
       "      <td>Standard Movement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000b70a00c8809f76b5234f81753264d9160c314cc5e6...</td>\n",
       "      <td>Passenger Vehicles</td>\n",
       "      <td>Reversing/Stopping</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     crash_record_id    vehicle_category  \\\n",
       "0  000013b0123279411e0ec856dae95ab9f0851764350b7f...  Passenger Vehicles   \n",
       "1  00002c0771fb6f2c70ba775b7f6b501608cadea85c1dd1...  Passenger Vehicles   \n",
       "2  00005696946846c8b8a1d378dba4e2a5ed84a9b2876fe0...              Trucks   \n",
       "3  000070ed7a6357c3298f5edc6fb7d5ce925a10f46660f3...  Passenger Vehicles   \n",
       "4  0000b70a00c8809f76b5234f81753264d9160c314cc5e6...  Passenger Vehicles   \n",
       "\n",
       "    maneuver_category  \n",
       "0  Reversing/Stopping  \n",
       "1   Standard Movement  \n",
       "2  Reversing/Stopping  \n",
       "3   Standard Movement  \n",
       "4  Reversing/Stopping  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Aggregating the vehicles data by crash_record_id\n",
    "vehicles_aggregated = vehicles_cleaned.groupby('crash_record_id').agg({\n",
    "    'vehicle_category': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan,  # Select the first mode\n",
    "    'maneuver_category': lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan   # Select the first mode\n",
    "}).reset_index()\n",
    "\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# View the aggregated vehicles data\n",
    "vehicles_aggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d3a5667b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deletes the vehicles_cleaned dataframe to clear up memory\n",
    "del vehicles_cleaned\n",
    "\n",
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1002cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes using an inner join\n",
    "merged_df = crashes_cleaned.merge(people_aggregated, on='crash_record_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f531a28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deletes the crashes_cleaned dataframe to clear up memory\n",
    "del crashes_cleaned\n",
    "\n",
    "# Perform garbage collection to free up memory by releasing unreferenced objects\n",
    "# This helps to manage memory usage, especially when working with large datasets or memory-intensive operations.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3a4ba94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(vehicles_aggregated, on='crash_record_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c57ec03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 705346 entries, 0 to 705345\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   crash_record_id           705346 non-null  object  \n",
      " 1   lighting_condition        705346 non-null  category\n",
      " 2   roadway_surface_cond      705346 non-null  category\n",
      " 3   speed_limit_category      705346 non-null  category\n",
      " 4   traffic_control_category  705346 non-null  category\n",
      " 5   road_category             705346 non-null  category\n",
      " 6   severity_category         705346 non-null  category\n",
      " 7   crash_cause_category      705346 non-null  category\n",
      " 8   time_of_day               705346 non-null  category\n",
      " 9   day_of_week               705346 non-null  category\n",
      " 10  season                    705346 non-null  category\n",
      " 11  sex                       705346 non-null  object  \n",
      " 12  age_group                 705346 non-null  object  \n",
      " 13  airbag_deployed           705346 non-null  object  \n",
      " 14  vehicle_category          705346 non-null  object  \n",
      " 15  maneuver_category         705346 non-null  object  \n",
      "dtypes: category(10), object(6)\n",
      "memory usage: 44.4+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8f555d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the columns (except 'crash_record_id') to category type\n",
    "merged_df[[col for col in merged_df.columns if col != 'crash_record_id']] = merged_df[[col for col in merged_df.columns if col != 'crash_record_id']].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d3e04660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id               object\n",
       "lighting_condition          category\n",
       "roadway_surface_cond        category\n",
       "speed_limit_category        category\n",
       "traffic_control_category    category\n",
       "road_category               category\n",
       "severity_category           category\n",
       "crash_cause_category        category\n",
       "time_of_day                 category\n",
       "day_of_week                 category\n",
       "season                      category\n",
       "sex                         category\n",
       "age_group                   category\n",
       "airbag_deployed             category\n",
       "vehicle_category            category\n",
       "maneuver_category           category\n",
       "dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the changes\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "679701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns = merged_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "723cafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string values in object columns to lowercase\n",
    "for col in merged_df.select_dtypes(include='object').columns:\n",
    "    merged_df[col] = merged_df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7fd27688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id             0\n",
       "lighting_condition          0\n",
       "roadway_surface_cond        0\n",
       "speed_limit_category        0\n",
       "traffic_control_category    0\n",
       "road_category               0\n",
       "severity_category           0\n",
       "crash_cause_category        0\n",
       "time_of_day                 0\n",
       "day_of_week                 0\n",
       "season                      0\n",
       "sex                         0\n",
       "age_group                   0\n",
       "airbag_deployed             0\n",
       "vehicle_category            0\n",
       "maneuver_category           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result\n",
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9ce39854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-serious    692193\n",
       "Serious         13153\n",
       "Name: severity_category, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['severity_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0149a994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_record_id</th>\n",
       "      <th>lighting_condition</th>\n",
       "      <th>roadway_surface_cond</th>\n",
       "      <th>speed_limit_category</th>\n",
       "      <th>traffic_control_category</th>\n",
       "      <th>road_category</th>\n",
       "      <th>severity_category</th>\n",
       "      <th>crash_cause_category</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>season</th>\n",
       "      <th>sex</th>\n",
       "      <th>age_group</th>\n",
       "      <th>airbag_deployed</th>\n",
       "      <th>vehicle_category</th>\n",
       "      <th>maneuver_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [crash_record_id, lighting_condition, roadway_surface_cond, speed_limit_category, traffic_control_category, road_category, severity_category, crash_cause_category, time_of_day, day_of_week, season, sex, age_group, airbag_deployed, vehicle_category, maneuver_category]\n",
       "Index: []"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate crash_record_id values\n",
    "merged_df[merged_df.duplicated(subset='crash_record_id', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8272c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts for crash_record_id:\n",
      "6c1659069e9c6285a650e70d6f9b574ed5f64c12888479093dfeef179c0344ec6d2057eae224b5c0d5dfc278c0a237f8c22543f07fdef2e4a95a3849871c9345    1\n",
      "b1099e521b2895496fd1e07adf0a7a8e5951524324b46fc6f1bb55a7cdd91151d6333fef63913fdfe3e75cd4ba47c9e8aba1a54d3af37ce3db416a6e5cf584f8    1\n",
      "b108accb609c2d67307d0c8c8c7d76be4792faed6554abe99f3780468e738e7493f8ee9089239c632fe500868edeefe1859fc5b2820fb5be47986b67c91985a8    1\n",
      "b108c6a47e34dd63f88879fd045946d5a95c90cae2910b8aab3f26cbd958601fcdf779fbbca2bd13650741a81064e7d5d19c87a776db46b32b5b3104c6708e69    1\n",
      "b108cdbe0649a3def6b17541148bff1b8a3b5289ac888ae34c615e96c2e745ec79fe85bed2fb00953f34362e65d01d30dc0a6aa62c140d7b7f89ea2c79a8ea6a    1\n",
      "                                                                                                                                   ..\n",
      "559aa3a1f71ea855105d98432e7b25145eaef80df6e74f369827fa2ae563859454e044ee166f7c0b0a3e8180eebdb0a49c6bb7a90187bd1ea5ce1a021133ecca    1\n",
      "559aa4fee2087be0d8d2e17dafe864982fd79989e57830b857f7c63efca7fc7b33081c0494e193daa37ef20e7fbaf8ae714d2c200d51a4802f1f036a6fd1d2f7    1\n",
      "559ac53d19b04f5b633bb9276bd1c0164a0c784633e847ca079d3772764c5812b0aac20f0576cecc62be96c87056642c26cf2155ab03d2bdbaf51480c021975b    1\n",
      "559ae327760dd75f6ea9ec458bed50cfedad8594d91bf5238268564f04062711e37198ca5ffe005ce5463a731832ab8605600435ee8738d464aeaf88bcfd4771    1\n",
      "ddb7c68c829fa774fa13b1a8bda817c7f9094a660447231ea7707c0ae19c6fab186f192076fd892cd3ad94987bd515ca2833a011008823e65a3e2b7a53544757    1\n",
      "Name: crash_record_id, Length: 705346, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for lighting_condition:\n",
      "daylight                  475786\n",
      "darkness, lighted road    151680\n",
      "darkness                   29623\n",
      "dusk                       21079\n",
      "unknown                    15502\n",
      "dawn                       11676\n",
      "Name: lighting_condition, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for roadway_surface_cond:\n",
      "dry                531856\n",
      "wet                 98380\n",
      "unknown             46647\n",
      "snow or slush       21772\n",
      "ice                  4713\n",
      "other                1743\n",
      "sand, mud, dirt       235\n",
      "Name: roadway_surface_cond, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for speed_limit_category:\n",
      "Medium    591857\n",
      "Low       106919\n",
      "High        6570\n",
      "Name: speed_limit_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for traffic_control_category:\n",
      "Other               390691\n",
      "Signal              231417\n",
      "Sign                 81904\n",
      "Markings & Lanes      1334\n",
      "Name: traffic_control_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for road_category:\n",
      "unknown                     531993\n",
      "multi-lane bidirectional    118871\n",
      "intersection                 22435\n",
      "other                        13682\n",
      "multi-lane one way            9204\n",
      "single-lane one way           9161\n",
      "Name: road_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for severity_category:\n",
      "Non-serious    692193\n",
      "Serious         13153\n",
      "Name: severity_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for crash_cause_category:\n",
      "Aggressive/Reckless Driving          374047\n",
      "Unknown/Other                        259358\n",
      "Driver's Condition/Experience         43426\n",
      "Environmental and Road Conditions     17505\n",
      "Distraction                           11010\n",
      "Name: crash_cause_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for time_of_day:\n",
      "Afternoon        303485\n",
      "Morning          184234\n",
      "Night (Early)    162107\n",
      "Night (Late)      55520\n",
      "Name: time_of_day, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for day_of_week:\n",
      "Fri     116745\n",
      "Thur    103776\n",
      "Wed     102327\n",
      "Tues    102057\n",
      "Sat     101733\n",
      "Mon      96151\n",
      "Sun      82557\n",
      "Name: day_of_week, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for season:\n",
      "Fall      194462\n",
      "Summer    184167\n",
      "Spring    165500\n",
      "Winter    161217\n",
      "Name: season, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for sex:\n",
      "female    368096\n",
      "male      336850\n",
      "other        400\n",
      "Name: sex, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for age_group:\n",
      "27-65    458230\n",
      "16-26    196623\n",
      "65+       28783\n",
      "1-15      21710\n",
      "Name: age_group, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for airbag_deployed:\n",
      "Not Deployed    618365\n",
      "Deployed         62049\n",
      "Unknown          24932\n",
      "Name: airbag_deployed, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for vehicle_category:\n",
      "Passenger Vehicles    586643\n",
      "SUVs                   59231\n",
      "Buses                  20677\n",
      "Other                  17793\n",
      "Trucks                 16545\n",
      "Motorcycles             4457\n",
      "Name: vehicle_category, dtype: int64\n",
      "--------------------------------------------------\n",
      "Value counts for maneuver_category:\n",
      "Standard Movement               486827\n",
      "Reversing/Stopping              147164\n",
      "Turn/Change of Direction         40861\n",
      "Special Cases                    18278\n",
      "Avoidance/Emergency Response     12216\n",
      "Name: maneuver_category, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each column in the DataFrame and print value counts for each feature\n",
    "for column in merged_df.columns:\n",
    "    print(f\"Value counts for {column}:\")\n",
    "    print(merged_df[column].value_counts())\n",
    "    print(\"-\" * 50)  # Optional separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c42ea13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crash_record_id             0\n",
       "lighting_condition          0\n",
       "roadway_surface_cond        0\n",
       "speed_limit_category        0\n",
       "traffic_control_category    0\n",
       "road_category               0\n",
       "severity_category           0\n",
       "crash_cause_category        0\n",
       "time_of_day                 0\n",
       "day_of_week                 0\n",
       "season                      0\n",
       "sex                         0\n",
       "age_group                   0\n",
       "airbag_deployed             0\n",
       "vehicle_category            0\n",
       "maneuver_category           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f8af22a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 639462 entries, 0 to 705345\n",
      "Data columns (total 16 columns):\n",
      " #   Column                    Non-Null Count   Dtype   \n",
      "---  ------                    --------------   -----   \n",
      " 0   crash_record_id           639462 non-null  object  \n",
      " 1   lighting_condition        639462 non-null  category\n",
      " 2   roadway_surface_cond      639462 non-null  category\n",
      " 3   speed_limit_category      639462 non-null  category\n",
      " 4   traffic_control_category  639462 non-null  category\n",
      " 5   road_category             639462 non-null  category\n",
      " 6   severity_category         639462 non-null  category\n",
      " 7   crash_cause_category      639462 non-null  category\n",
      " 8   time_of_day               639462 non-null  category\n",
      " 9   day_of_week               639462 non-null  category\n",
      " 10  season                    639462 non-null  category\n",
      " 11  sex                       639462 non-null  category\n",
      " 12  age_group                 639462 non-null  category\n",
      " 13  airbag_deployed           639462 non-null  category\n",
      " 14  vehicle_category          639462 non-null  category\n",
      " 15  maneuver_category         639462 non-null  category\n",
      "dtypes: category(15), object(1)\n",
      "memory usage: 18.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# List of columns you want to clean\n",
    "columns_to_clean = ['airbag_deployed', 'roadway_surface_cond', 'lighting_condition']  # replace with your actual column names\n",
    "\n",
    "# Iterate over each column in the list\n",
    "for column in columns_to_clean:\n",
    "    # Check for unwanted values in the current column and remove rows\n",
    "    merged_df = merged_df[~merged_df[column].str.contains(\n",
    "        'unknown|not applicable|other object|unknown/other', case=False, na=False)]\n",
    "\n",
    "# Verify the changes\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3b45e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'crash_record_id' column from the merged dataframe\n",
    "merged_df = merged_df.drop('crash_record_id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d75b0376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(639462, 15)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b6151e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lighting_condition          0\n",
       "roadway_surface_cond        0\n",
       "speed_limit_category        0\n",
       "traffic_control_category    0\n",
       "road_category               0\n",
       "severity_category           0\n",
       "crash_cause_category        0\n",
       "time_of_day                 0\n",
       "day_of_week                 0\n",
       "season                      0\n",
       "sex                         0\n",
       "age_group                   0\n",
       "airbag_deployed             0\n",
       "vehicle_category            0\n",
       "maneuver_category           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9dd4703c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-serious    0.980882\n",
       "Serious        0.019118\n",
       "Name: severity_category, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['severity_category'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a44078",
   "metadata": {},
   "source": [
    "From the above output, we see that the classes are greatly imbalanced. This will be something I will need to address during the modeling phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ba404734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping classes into two groups for binary classification: 0 and 1\n",
    "merged_df.severity_category.replace({\n",
    "    'Serious' : 1,\n",
    "    'Non-serious' : 0},\n",
    "    inplace = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8238702",
   "metadata": {},
   "source": [
    "## Very large data so going to take a subset for the feature importances modeling portion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8c820e",
   "metadata": {},
   "source": [
    "Chose 10% sampling as this provided a subset of just under 50k records. This optimizes computational efficiency and memory storage, while also ensuring there is adequate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cbcb9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the desired sample size (e.g., 15% of the dataset)\n",
    "sample_size = 0.15 \n",
    "\n",
    "# Perform stratified sampling to retain class distribution\n",
    "crashes_finalized_df, _ = train_test_split(\n",
    "    merged_df, \n",
    "    test_size=1-sample_size,  # Retain only `sample_size` fraction\n",
    "    stratify=merged_df['severity_category'], \n",
    "    random_state=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2aa50b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 95919 entries, 517577 to 154697\n",
      "Data columns (total 15 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   lighting_condition        95919 non-null  category\n",
      " 1   roadway_surface_cond      95919 non-null  category\n",
      " 2   speed_limit_category      95919 non-null  category\n",
      " 3   traffic_control_category  95919 non-null  category\n",
      " 4   road_category             95919 non-null  category\n",
      " 5   severity_category         95919 non-null  int64   \n",
      " 6   crash_cause_category      95919 non-null  category\n",
      " 7   time_of_day               95919 non-null  category\n",
      " 8   day_of_week               95919 non-null  category\n",
      " 9   season                    95919 non-null  category\n",
      " 10  sex                       95919 non-null  category\n",
      " 11  age_group                 95919 non-null  category\n",
      " 12  airbag_deployed           95919 non-null  category\n",
      " 13  vehicle_category          95919 non-null  category\n",
      " 14  maneuver_category         95919 non-null  category\n",
      "dtypes: category(14), int64(1)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "crashes_finalized_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1d0a864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution:\n",
      "0    0.980882\n",
      "1    0.019118\n",
      "Name: severity_category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Confirm the class distribution remains the same\n",
    "print(\"Original class distribution:\")\n",
    "print(merged_df['severity_category'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e54f0a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sampled class distribution:\n",
      "0    0.98088\n",
      "1    0.01912\n",
      "Name: severity_category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSampled class distribution:\")\n",
    "print(crashes_finalized_df['severity_category'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d6d4b",
   "metadata": {},
   "source": [
    "## Exporting dataset to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee7b2",
   "metadata": {},
   "source": [
    "I save the cleaned dataframe as a csv and manually upload it to kaggle for use in the rest of the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b53e400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and finalized DataFrame to a CSV file in the 'data' directory\n",
    "crashes_finalized_df.to_csv('./data/crashes_finalized_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
